{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.036828,
     "end_time": "2019-11-15T00:50:24.898961",
     "exception": false,
     "start_time": "2019-11-15T00:50:24.862133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48054,
     "status": "ok",
     "timestamp": 1573636583219,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "2mU_ZLUH5jz4",
    "outputId": "c4cbc445-56dd-49df-de87-c34e18da5f6c",
    "papermill": {
     "duration": 16.20343,
     "end_time": "2019-11-15T00:50:41.116449",
     "exception": false,
     "start_time": "2019-11-15T00:50:24.913019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "from mricode.utils import log_textfile, createPath, data_generator\n",
    "from mricode.utils import copy_colab\n",
    "from mricode.utils import return_iter\n",
    "from mricode.utils import return_csv\n",
    "\n",
    "from mricode.models.SimpleCNN import SimpleCNN\n",
    "from mricode.models.DenseNet_NoDict import MyDenseNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow import nn\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras.engine.base_layer import InputSpec\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 28.805972,
     "end_time": "2019-11-15T00:51:09.937629",
     "exception": false,
     "start_time": "2019-11-15T00:50:41.131657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nH4XzW8C5yhH",
    "papermill": {
     "duration": 0.058335,
     "end_time": "2019-11-15T00:51:10.010887",
     "exception": false,
     "start_time": "2019-11-15T00:51:09.952552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_output = './output/'\n",
    "path_tfrecords = '/data2/res64/down/'\n",
    "path_csv = '/data2/csv/'\n",
    "filename_res = {'train': 'intell_residual_train.csv', 'val': 'intell_residual_valid.csv', 'test': 'intell_residual_test.csv'}\n",
    "filename_final = filename_res\n",
    "sample_size = 'site16_allimages'\n",
    "batch_size = 8\n",
    "onlyt1 = False\n",
    "modelname = 'densenet_allimages64'\n",
    "Model = SimpleCNN\n",
    "Model = MyDenseNet\n",
    "t1_mean=1.3779395849814497\n",
    "t1_std=3.4895845243139503\n",
    "t2_mean=2.22435586968901\n",
    "t2_std=5.07708743178319\n",
    "ad_mean=1.3008901218593748e-05\n",
    "ad_std=0.009966655860940228\n",
    "fa_mean=0.0037552628409334037\n",
    "fa_std=0.012922319568740915\n",
    "md_mean=9.827903909139596e-06\n",
    "md_std=0.009956973204022659\n",
    "rd_mean=8.237404999587111e-06\n",
    "rd_std=0.009954672598675338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "createPath(path_output + modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4520,
     "status": "ok",
     "timestamp": 1573636640714,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "B-rSS9vT6TuF",
    "outputId": "f5b43cbc-7d59-4c7c-b6d6-78882b1ef3e1",
    "papermill": {
     "duration": 0.475844,
     "end_time": "2019-11-15T00:51:10.501145",
     "exception": false,
     "start_time": "2019-11-15T00:51:10.025301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fa4b1d4ec80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fa4b1d4ec80> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter, test_iter = return_iter(path_tfrecords, sample_size, batch_size, onlyt1=onlyt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJ0VnyEzBAes",
    "papermill": {
     "duration": 0.061717,
     "end_time": "2019-11-15T00:51:10.578019",
     "exception": false,
     "start_time": "2019-11-15T00:51:10.516302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "  t1_mean = 0.\n",
    "  t1_std = 0.\n",
    "  t2_mean = 0.\n",
    "  t2_std = 0.\n",
    "  ad_mean = 0.\n",
    "  ad_std = 0.\n",
    "  fa_mean = 0.\n",
    "  fa_std = 0.\n",
    "  md_mean = 0.\n",
    "  md_std = 0.\n",
    "  rd_mean = 0.\n",
    "  rd_std = 0.\n",
    "  n = 0.\n",
    "  for b in train_iter:\n",
    "      t1_mean += np.mean(b['t1'])\n",
    "      t1_std += np.std(b['t1'])\n",
    "      t2_mean += np.mean(b['t2'])\n",
    "      t2_std += np.std(b['t2'])\n",
    "      a = np.asarray(b['ad'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      ad_mean += np.mean(a)\n",
    "      ad_std += np.std(a)\n",
    "      a = np.asarray(b['fa'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      fa_mean += np.mean(a)\n",
    "      fa_std += np.std(a)\n",
    "      a = np.asarray(b['md'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      md_mean += np.mean(a)\n",
    "      md_std += np.std(a)\n",
    "      a = np.asarray(b['rd'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      rd_mean += np.mean(a)\n",
    "      rd_std += np.std(a)\n",
    "      n += np.asarray(b['t1']).shape[0]\n",
    "\n",
    "  t1_mean /= n\n",
    "  t1_std /= n\n",
    "  t2_mean /= n\n",
    "  t2_std /= n\n",
    "  ad_mean /= n\n",
    "  ad_std /= n\n",
    "  fa_mean /= n\n",
    "  fa_std /= n\n",
    "  md_mean /= n\n",
    "  md_std /= n\n",
    "  rd_mean /= n\n",
    "  rd_std /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2924,
     "status": "ok",
     "timestamp": 1573636640716,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "mSwhEqDPM_r8",
    "outputId": "14a4ac0b-9587-4776-fc64-1e008f0c5c31",
    "papermill": {
     "duration": 0.054036,
     "end_time": "2019-11-15T00:51:10.646744",
     "exception": false,
     "start_time": "2019-11-15T00:51:10.592708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3779395849814497,\n",
       " 3.4895845243139503,\n",
       " 2.22435586968901,\n",
       " 5.07708743178319,\n",
       " 1.3008901218593748e-05,\n",
       " 0.009966655860940228,\n",
       " 0.0037552628409334037,\n",
       " 0.012922319568740915,\n",
       " 9.827903909139596e-06,\n",
       " 0.009956973204022659,\n",
       " 8.237404999587111e-06,\n",
       " 0.009954672598675338)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_mean, t1_std, t2_mean, t2_std, ad_mean, ad_std, fa_mean, fa_std, md_mean, md_std, rd_mean, rd_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAyV5ktlA6K1",
    "papermill": {
     "duration": 0.427157,
     "end_time": "2019-11-15T00:51:11.088890",
     "exception": false,
     "start_time": "2019-11-15T00:51:10.661733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, norm_dict = return_csv(path_csv, filename_final, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectkey</th>\n",
       "      <th>age</th>\n",
       "      <th>female</th>\n",
       "      <th>race.ethnicity</th>\n",
       "      <th>high.educ</th>\n",
       "      <th>income</th>\n",
       "      <th>married</th>\n",
       "      <th>abcd_site</th>\n",
       "      <th>vol</th>\n",
       "      <th>height</th>\n",
       "      <th>...</th>\n",
       "      <th>nihtbx_fluidcomp_uncorrected_norm</th>\n",
       "      <th>nihtbx_cryst_uncorrected_norm</th>\n",
       "      <th>nihtbx_pattern_uncorrected_norm</th>\n",
       "      <th>nihtbx_picture_uncorrected_norm</th>\n",
       "      <th>nihtbx_list_uncorrected_norm</th>\n",
       "      <th>nihtbx_flanker_uncorrected_norm</th>\n",
       "      <th>nihtbx_picvocab_uncorrected_norm</th>\n",
       "      <th>nihtbx_cardsort_uncorrected_norm</th>\n",
       "      <th>nihtbx_totalcomp_uncorrected_norm</th>\n",
       "      <th>nihtbx_reading_uncorrected_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NDARINV139PZ31D</td>\n",
       "      <td>131</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.021648</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254797</td>\n",
       "      <td>0.420072</td>\n",
       "      <td>-0.246679</td>\n",
       "      <td>-0.281764</td>\n",
       "      <td>0.003202</td>\n",
       "      <td>1.159853</td>\n",
       "      <td>0.671312</td>\n",
       "      <td>0.410626</td>\n",
       "      <td>0.297575</td>\n",
       "      <td>-0.013243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NDARINVK75DL4FA</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.524229</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.028648</td>\n",
       "      <td>1.139822</td>\n",
       "      <td>0.175448</td>\n",
       "      <td>0.213122</td>\n",
       "      <td>0.895299</td>\n",
       "      <td>1.280720</td>\n",
       "      <td>-0.758987</td>\n",
       "      <td>1.091243</td>\n",
       "      <td>1.241000</td>\n",
       "      <td>2.679676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NDARINVVGLF61WZ</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2.063406</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808336</td>\n",
       "      <td>-0.606098</td>\n",
       "      <td>0.495850</td>\n",
       "      <td>-0.029758</td>\n",
       "      <td>0.271333</td>\n",
       "      <td>1.038995</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>1.024084</td>\n",
       "      <td>0.308836</td>\n",
       "      <td>-1.000929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NDARINVXN1RT0M9</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.971421</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.062271</td>\n",
       "      <td>-0.931299</td>\n",
       "      <td>-0.086776</td>\n",
       "      <td>-0.337880</td>\n",
       "      <td>-0.650198</td>\n",
       "      <td>0.889379</td>\n",
       "      <td>-0.161298</td>\n",
       "      <td>0.383733</td>\n",
       "      <td>-0.564930</td>\n",
       "      <td>-1.423895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NDARINVPAJ2K2CD</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1.931861</td>\n",
       "      <td>58.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.341213</td>\n",
       "      <td>-0.852545</td>\n",
       "      <td>2.480674</td>\n",
       "      <td>-0.173929</td>\n",
       "      <td>0.205938</td>\n",
       "      <td>0.481268</td>\n",
       "      <td>-0.259162</td>\n",
       "      <td>0.724245</td>\n",
       "      <td>0.669324</td>\n",
       "      <td>-1.111902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subjectkey  age  female  race.ethnicity  high.educ  income  married  \\\n",
       "0  NDARINV139PZ31D  131       1               4         15       4        0   \n",
       "1  NDARINVK75DL4FA  112       0               2         19       8        3   \n",
       "2  NDARINVVGLF61WZ  118       0               0         19       9        0   \n",
       "3  NDARINVXN1RT0M9  115       0               0         18       9        0   \n",
       "4  NDARINVPAJ2K2CD  130       0               0         18       9        0   \n",
       "\n",
       "   abcd_site       vol     height  ...  nihtbx_fluidcomp_uncorrected_norm  \\\n",
       "0          4 -1.021648  63.000000  ...                           0.254797   \n",
       "1          3  0.524229  54.000000  ...                           1.028648   \n",
       "2         20  2.063406  56.250000  ...                           0.808336   \n",
       "3          9  0.971421  57.333333  ...                          -0.062271   \n",
       "4         14  1.931861  58.500000  ...                           1.341213   \n",
       "\n",
       "   nihtbx_cryst_uncorrected_norm  nihtbx_pattern_uncorrected_norm  \\\n",
       "0                       0.420072                        -0.246679   \n",
       "1                       1.139822                         0.175448   \n",
       "2                      -0.606098                         0.495850   \n",
       "3                      -0.931299                        -0.086776   \n",
       "4                      -0.852545                         2.480674   \n",
       "\n",
       "   nihtbx_picture_uncorrected_norm  nihtbx_list_uncorrected_norm  \\\n",
       "0                        -0.281764                      0.003202   \n",
       "1                         0.213122                      0.895299   \n",
       "2                        -0.029758                      0.271333   \n",
       "3                        -0.337880                     -0.650198   \n",
       "4                        -0.173929                      0.205938   \n",
       "\n",
       "   nihtbx_flanker_uncorrected_norm  nihtbx_picvocab_uncorrected_norm  \\\n",
       "0                         1.159853                          0.671312   \n",
       "1                         1.280720                         -0.758987   \n",
       "2                         1.038995                          0.001815   \n",
       "3                         0.889379                         -0.161298   \n",
       "4                         0.481268                         -0.259162   \n",
       "\n",
       "   nihtbx_cardsort_uncorrected_norm  nihtbx_totalcomp_uncorrected_norm  \\\n",
       "0                          0.410626                           0.297575   \n",
       "1                          1.091243                           1.241000   \n",
       "2                          1.024084                           0.308836   \n",
       "3                          0.383733                          -0.564930   \n",
       "4                          0.724245                           0.669324   \n",
       "\n",
       "   nihtbx_reading_uncorrected_norm  \n",
       "0                        -0.013243  \n",
       "1                         2.679676  \n",
       "2                        -1.000929  \n",
       "3                        -1.423895  \n",
       "4                        -1.111902  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('/data/home/benedikt_d_schifferer/data_T2_lowerres_cropped128/' + '*.gz') \n",
    "all_files += glob.glob('/data/home/benedikt_d_schifferer/data_T1_lowerres_cropped128/' + '*.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = data_generator(list(train_df['subjectkey'].values), all_files, batch_size)\n",
    "train_iter = data_generator(list(val_df['subjectkey'].values), all_files, batch_size)\n",
    "train_iter = data_generator(list(test_df['subjectkey'].values), all_files, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "t2 = (batch['t2']-t2_mean)/t2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "subjectid = decoder(batch['subjectid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NDARINVKKBJMBLJ', 'NDARINVNKYNEYLV', 'NDARINV2YBXVCKR',\n",
       "       'NDARINVAPL25X3B', 'NDARINVJCE41W2C', 'NDARINV52AU0VED',\n",
       "       'NDARINVDBHYPZ31', 'NDARINVUPDUETFG'], dtype='<U15')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjectid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = get_labels(train_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "X = tf.concat([t1, t2], axis=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3147,
     "status": "ok",
     "timestamp": 1573636641787,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "0xYy4XUyVBeC",
    "outputId": "a6a19d59-31ff-4064-f1dc-952befcce4b3",
    "papermill": {
     "duration": 0.058163,
     "end_time": "2019-11-15T00:51:11.162494",
     "exception": false,
     "start_time": "2019-11-15T00:51:11.104331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BMI': {'mean': 18.681548127052135, 'std': 4.193043131845343},\n",
       " 'age': {'mean': 119.00325844623563, 'std': 7.479129774017182},\n",
       " 'height': {'mean': 55.25173666322929, 'std': 3.152756181679028},\n",
       " 'nihtbx_cardsort_uncorrected': {'mean': 0.01902727147573316,\n",
       "  'std': 0.92710655806542},\n",
       " 'nihtbx_cryst_uncorrected': {'mean': 0.007018628014748754,\n",
       "  'std': 0.7845373584638602},\n",
       " 'nihtbx_flanker_uncorrected': {'mean': 0.02188780794049048,\n",
       "  'std': 0.9070917080607726},\n",
       " 'nihtbx_fluidcomp_uncorrected': {'mean': 0.020178243913565427,\n",
       "  'std': 0.86606123778624},\n",
       " 'nihtbx_list_uncorrected': {'mean': 0.00625176016120734,\n",
       "  'std': 0.8898550695735616},\n",
       " 'nihtbx_pattern_uncorrected': {'mean': 0.020721412569885096,\n",
       "  'std': 0.9486618556882954},\n",
       " 'nihtbx_picture_uncorrected': {'mean': 0.0005782175223803825,\n",
       "  'std': 0.9577989703304521},\n",
       " 'nihtbx_picvocab_uncorrected': {'mean': 0.0068509109986280275,\n",
       "  'std': 0.8038465951211212},\n",
       " 'nihtbx_reading_uncorrected': {'mean': 0.0027847502208883574,\n",
       "  'std': 0.8572703419965433},\n",
       " 'nihtbx_totalcomp_uncorrected': {'mean': 0.016971206631452577,\n",
       "  'std': 0.7866686549175077},\n",
       " 'vol': {'mean': 0.013939557398902416, 'std': 1.001699783217792},\n",
       " 'weight': {'mean': 81.91536668875837, 'std': 23.321845390211728}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTk95ptFV5oN",
    "papermill": {
     "duration": 0.055596,
     "end_time": "2019-11-15T00:51:11.234329",
     "exception": false,
     "start_time": "2019-11-15T00:51:11.178733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = {'female': 2, 'race.ethnicity': 5, 'high.educ_group': 4, 'income_group': 8, 'married': 6}\n",
    "num_cols = [x for x in list(val_df.columns) if '_norm' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hROMApYiDagm",
    "papermill": {
     "duration": 0.096063,
     "end_time": "2019-11-15T00:51:11.345922",
     "exception": false,
     "start_time": "2019-11-15T00:51:11.249859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict):\n",
    "  for col in num_cols:\n",
    "    tmp_col = col\n",
    "    tmp_std = norm_dict[tmp_col.replace('_norm','')]['std']\n",
    "    tmp_y_true = tf.cast(y_true[col], tf.float32).numpy()\n",
    "    tmp_y_pred = np.squeeze(y_pred[col].numpy())\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    else:\n",
    "      out_loss[tmp_col] += np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "    else:\n",
    "      out_acc[tmp_col] += np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "  for col in list(cat_cols.keys()):\n",
    "    tmp_col = col\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    else:\n",
    "      out_loss[tmp_col] += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()\n",
    "    else:\n",
    "      out_acc[tmp_col] += tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()    \n",
    "  return(out_loss, out_acc)\n",
    "\n",
    "def format_output(out_loss, out_acc, n, cols, print_bl=False):\n",
    "  loss = 0\n",
    "  acc = 0\n",
    "  output = []\n",
    "  for col in cols:\n",
    "    output.append([col, out_loss[col]/n, out_acc[col]/n])\n",
    "    loss += out_loss[col]/n\n",
    "    acc += out_acc[col]/n\n",
    "  df = pd.DataFrame(output)\n",
    "  df.columns = ['name', 'loss', 'acc']\n",
    "  if print_bl:\n",
    "    print(df)\n",
    "  return(loss, acc, df)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, y, model, optimizer, cat_cols, num_cols):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(X)\n",
    "    i = 0\n",
    "    loss = tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for i in range(1,len(num_cols)):\n",
    "      loss += tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for col in list(cat_cols.keys()):\n",
    "      loss += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y[col]), tf.squeeze(predictions[col]))\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]\n",
    "  with tf.control_dependencies(mean_std):\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return(y, predictions, loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(X, y, model):\n",
    "  predictions = model(X)\n",
    "  return(y, predictions)\n",
    "\n",
    "def epoch(data_iter, df, model, optimizer, cat_cols, num_cols, norm_dict):\n",
    "  out_loss = {}\n",
    "  out_acc = {}\n",
    "  n = 0.\n",
    "  n_batch = 0.\n",
    "  total_time_dataload = 0.\n",
    "  total_time_model = 0.\n",
    "  start_time = time.time()\n",
    "  for batch in data_iter:\n",
    "    total_time_dataload += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "    t2 = (batch['t2']-t2_mean)/t2_std\n",
    "    ad = batch['ad']\n",
    "    ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "    ad = (ad-ad_mean)/ad_std\n",
    "    fa = batch['fa']\n",
    "    fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "    fa = (fa-fa_mean)/fa_std\n",
    "    md = batch['md']\n",
    "    md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "    md = (md-md_mean)/md_std\n",
    "    rd = batch['rd']\n",
    "    rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "    rd = (rd-rd_mean)/rd_std\n",
    "    subjectid = decoder(batch['subjectid'])\n",
    "    y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "    X = tf.concat([t1, t2], axis=4)\n",
    "    #X = tf.concat([t1, t2], axis=4)\n",
    "    if optimizer != None:\n",
    "      y_true, y_pred, loss = train_step(X, y, model, optimizer, cat_cols, num_cols)\n",
    "    else:\n",
    "      y_true, y_pred = test_step(X, y, model)\n",
    "    out_loss, out_acc = calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict)\n",
    "    n += X.shape[0]\n",
    "    n_batch += 1\n",
    "    if (n_batch % 10) == 0:\n",
    "      print(n_batch)\n",
    "    total_time_model += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "  return (out_loss, out_acc, n, total_time_model, total_time_dataload)\n",
    "\n",
    "def get_labels(df, subjectid, cols = ['nihtbx_fluidcomp_uncorrected_norm']):\n",
    "  subjects_df = pd.DataFrame(subjectid)\n",
    "  result_df = pd.merge(subjects_df, df, left_on=0, right_on='subjectkey', how='left')\n",
    "  output = {}\n",
    "  for col in cols:\n",
    "    output[col] = np.asarray(result_df[col].values)\n",
    "  return output\n",
    "\n",
    "def best_val(df_best, df_val, df_test, e):\n",
    "  df_best = pd.merge(df_best, df_val, how='left', left_on='name', right_on='name')\n",
    "  df_best = pd.merge(df_best, df_test, how='left', left_on='name', right_on='name')\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_epochs'] = e\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_epochs'] = e\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_test'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_test']\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_val'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_val']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best = df_best.drop(['cur_loss_val', 'cur_acc_val', 'cur_loss_test', 'cur_acc_test'], axis=1)\n",
    "  return(df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46355,
     "status": "ok",
     "timestamp": 1573636701683,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "RV24LE3k-00n",
    "outputId": "067125be-6ce2-40c9-998e-62f800ccb2f1",
    "papermill": {
     "duration": 21265.754478,
     "end_time": "2019-11-15T06:45:37.115987",
     "exception": false,
     "start_time": "2019-11-15T00:51:11.361509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'female': 2, 'married': 6, 'race.ethnicity': 5, 'high.educ_group': 4, 'income_group': 8}\n",
      "['BMI_norm', 'age_norm', 'vol_norm', 'weight_norm', 'height_norm', 'nihtbx_fluidcomp_uncorrected_norm', 'nihtbx_cryst_uncorrected_norm', 'nihtbx_pattern_uncorrected_norm', 'nihtbx_picture_uncorrected_norm', 'nihtbx_list_uncorrected_norm', 'nihtbx_flanker_uncorrected_norm', 'nihtbx_picvocab_uncorrected_norm', 'nihtbx_cardsort_uncorrected_norm', 'nihtbx_totalcomp_uncorrected_norm', 'nihtbx_reading_uncorrected_norm']\n",
      "Epochs: 0\n",
      "10.0\n",
      "20.0\n",
      "30.0\n",
      "40.0\n",
      "50.0\n",
      "60.0\n",
      "70.0\n",
      "10.0\n",
      "10.0\n",
      "                                 name  best_loss_test  best_acc_test\n",
      "0                      race.ethnicity        0.105219       0.778689\n",
      "1                        income_group        0.248143       0.245902\n",
      "2                             married        0.079374       0.860656\n",
      "3                              female        0.099409       0.540984\n",
      "4                     high.educ_group        0.050456       0.926230\n",
      "5                            BMI_norm        1.253056      22.030740\n",
      "6                            age_norm        1.092049      61.086374\n",
      "7                            vol_norm        1.497219       1.502313\n",
      "8                         weight_norm        1.140161     620.143058\n",
      "9                         height_norm        0.863040       8.578510\n",
      "10  nihtbx_fluidcomp_uncorrected_norm        0.779097       0.584371\n",
      "11      nihtbx_cryst_uncorrected_norm        1.000668       0.615910\n",
      "12    nihtbx_pattern_uncorrected_norm        0.718599       0.646710\n",
      "13    nihtbx_picture_uncorrected_norm        0.799675       0.733605\n",
      "14       nihtbx_list_uncorrected_norm        0.833429       0.659944\n",
      "15    nihtbx_flanker_uncorrected_norm        0.849038       0.698602\n",
      "16   nihtbx_picvocab_uncorrected_norm        1.085368       0.701331\n",
      "17   nihtbx_cardsort_uncorrected_norm        0.743972       0.639464\n",
      "18  nihtbx_totalcomp_uncorrected_norm        0.876483       0.542409\n",
      "19    nihtbx_reading_uncorrected_norm        0.791616       0.581769\n",
      "                                 name  best_loss_val  best_acc_val\n",
      "0                      race.ethnicity       0.082871      0.786885\n",
      "1                        income_group       0.217054      0.360656\n",
      "2                             married       0.069505      0.819672\n",
      "3                              female       0.095974      0.491803\n",
      "4                     high.educ_group       0.059225      0.836066\n",
      "5                            BMI_norm       0.900162     15.826308\n",
      "6                            age_norm       1.220833     68.290199\n",
      "7                            vol_norm       1.420239      1.425071\n",
      "8                         weight_norm       0.805814    438.289287\n",
      "9                         height_norm       0.869148      8.639219\n",
      "10  nihtbx_fluidcomp_uncorrected_norm       0.867403      0.650606\n",
      "11      nihtbx_cryst_uncorrected_norm       0.940583      0.578928\n",
      "12    nihtbx_pattern_uncorrected_norm       0.846742      0.762033\n",
      "13    nihtbx_picture_uncorrected_norm       1.049455      0.962748\n",
      "14       nihtbx_list_uncorrected_norm       0.832866      0.659499\n",
      "15    nihtbx_flanker_uncorrected_norm       1.021991      0.840910\n",
      "16   nihtbx_picvocab_uncorrected_norm       0.987635      0.638179\n",
      "17   nihtbx_cardsort_uncorrected_norm       0.859598      0.738847\n",
      "18  nihtbx_totalcomp_uncorrected_norm       0.791572      0.489862\n",
      "19    nihtbx_reading_uncorrected_norm       0.802224      0.589564\n",
      "Epoch 0, Loss: 62.284, Accuracy: 1292.974, Val Loss: 14.741, Val Accuracy: 542.676, Time Model: 87.781, Time Data: 1.938\n",
      "Epochs: 1\n",
      "10.0\n",
      "20.0\n",
      "30.0\n",
      "40.0\n",
      "50.0\n",
      "60.0\n",
      "70.0\n",
      "10.0\n",
      "10.0\n",
      "                                 name  best_loss_test  best_acc_test\n",
      "0                      race.ethnicity        0.105219       0.778689\n",
      "1                        income_group        0.248143       0.245902\n",
      "2                             married        0.079374       0.860656\n",
      "3                              female        0.095483       0.540984\n",
      "4                     high.educ_group        0.050456       0.926230\n",
      "5                            BMI_norm        0.965826      16.980773\n",
      "6                            age_norm        1.087002      60.804043\n",
      "7                            vol_norm        1.074662       1.078318\n",
      "8                         weight_norm        1.040657     566.022285\n",
      "9                         height_norm        0.831933       8.269310\n",
      "10  nihtbx_fluidcomp_uncorrected_norm        0.779097       0.584371\n",
      "11      nihtbx_cryst_uncorrected_norm        1.000668       0.615910\n",
      "12    nihtbx_pattern_uncorrected_norm        0.714564       0.643079\n",
      "13    nihtbx_picture_uncorrected_norm        0.799675       0.733605\n",
      "14       nihtbx_list_uncorrected_norm        0.796350       0.630584\n",
      "15    nihtbx_flanker_uncorrected_norm        0.791070       0.650905\n",
      "16   nihtbx_picvocab_uncorrected_norm        1.103953       0.713340\n",
      "17   nihtbx_cardsort_uncorrected_norm        0.736231       0.632810\n",
      "18  nihtbx_totalcomp_uncorrected_norm        0.840326       0.520034\n",
      "19    nihtbx_reading_uncorrected_norm        0.791616       0.581769\n",
      "                                 name  best_loss_val  best_acc_val\n",
      "0                      race.ethnicity       0.082871      0.786885\n",
      "1                        income_group       0.217054      0.360656\n",
      "2                             married       0.069505      0.819672\n",
      "3                              female       0.091765      0.491803\n",
      "4                     high.educ_group       0.059225      0.836066\n",
      "5                            BMI_norm       0.714016     12.553551\n",
      "6                            age_norm       1.209163     67.637407\n",
      "7                            vol_norm       1.058894      1.062497\n",
      "8                         weight_norm       0.753191    409.667168\n",
      "9                         height_norm       0.812094      8.072105\n",
      "10  nihtbx_fluidcomp_uncorrected_norm       0.867403      0.650606\n",
      "11      nihtbx_cryst_uncorrected_norm       0.940583      0.578928\n",
      "12    nihtbx_pattern_uncorrected_norm       0.837394      0.753620\n",
      "13    nihtbx_picture_uncorrected_norm       1.049455      0.962748\n",
      "14       nihtbx_list_uncorrected_norm       0.751665      0.595200\n",
      "15    nihtbx_flanker_uncorrected_norm       0.950416      0.782017\n",
      "16   nihtbx_picvocab_uncorrected_norm       0.987296      0.637961\n",
      "17   nihtbx_cardsort_uncorrected_norm       0.834746      0.717486\n",
      "18  nihtbx_totalcomp_uncorrected_norm       0.782884      0.484486\n",
      "19    nihtbx_reading_uncorrected_norm       0.802224      0.589564\n",
      "Epoch 1, Loss: 58.902, Accuracy: 947.637, Val Loss: 13.932, Val Accuracy: 509.087, Time Model: 52.408, Time Data: 1.764\n",
      "Epochs: 2\n",
      "10.0\n",
      "20.0\n",
      "30.0\n",
      "40.0\n"
     ]
    }
   ],
   "source": [
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "template = 'Epoch {0}, Loss: {1:.3f}, Accuracy: {2:.3f}, Val Loss: {3:.3f}, Val Accuracy: {4:.3f}, Time Model: {5:.3f}, Time Data: {6:.3f}'\n",
    "for col in [0]:\n",
    "  log_textfile(path_output + modelname + '/log' + '.log', cat_cols),\n",
    "  log_textfile(path_output + modelname + '/log' + '.log', num_cols)\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "  model = Model(cat_cols, num_cols)\n",
    "  df_best = None\n",
    "  for e in range(20):\n",
    "    log_textfile(path_output + modelname + '/log' + '.log', 'Epochs: ' + str(e))\n",
    "    loss = tf.Variable(0.)\n",
    "    acc = tf.Variable(0.) \n",
    "    val_loss = tf.Variable(0.)\n",
    "    val_acc = tf.Variable(0.)\n",
    "    test_loss = tf.Variable(0.)\n",
    "    test_acc = tf.Variable(0.)\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "    train_out_loss, train_out_acc, n, time_model, time_data = epoch(train_iter, train_df, model, optimizer, cat_cols, num_cols, norm_dict)\n",
    "    tf.keras.backend.set_learning_phase(False)\n",
    "    val_out_loss, val_out_acc, n, _, _ = epoch(val_iter, val_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "    test_out_loss, test_out_acc, n, _, _ = epoch(test_iter, test_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "    loss, acc, _ = format_output(train_out_loss, train_out_acc, n, list(cat_cols.keys())+num_cols)\n",
    "    val_loss, val_acc, df_val = format_output(val_out_loss, val_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "    test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "    df_val.columns = ['name', 'cur_loss_val', 'cur_acc_val']\n",
    "    df_test.columns = ['name', 'cur_loss_test', 'cur_acc_test']\n",
    "    if e == 0:\n",
    "      df_best = pd.merge(df_test, df_val, how='left', left_on='name', right_on='name')\n",
    "      df_best['best_acc_epochs'] = 0\n",
    "      df_best['best_loss_epochs'] = 0\n",
    "      df_best.columns = ['name', 'best_loss_test', 'best_acc_test', 'best_loss_val', 'best_acc_val', 'best_acc_epochs', 'best_loss_epochs']\n",
    "    df_best = best_val(df_best, df_val, df_test, e)\n",
    "    print(df_best[['name', 'best_loss_test', 'best_acc_test']])\n",
    "    print(df_best[['name', 'best_loss_val', 'best_acc_val']])\n",
    "    log_textfile(path_output + modelname + '/log' + '.log', template.format(e, loss, acc, val_loss, val_acc, time_model, time_data))\n",
    "    if e in [10, 15]:\n",
    "      optimizer.lr = optimizer.lr/3\n",
    "      log_textfile(path_output + modelname + '/log' + '.log', 'Learning rate: ' + str(optimizer.lr))\n",
    "    df_best.to_csv(path_output +  modelname + '/df_best' + str(e) + '.csv')\n",
    "    df_best.to_csv(path_output +  modelname + '/df_best' + '.csv')\n",
    "    model.save_weights(path_output + modelname + '/checkpoints/' + str(e) + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('final_output_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(64,64,64,2), name='inputlayer123')\n",
    "a = model(inputs)['female']\n",
    "mm = tf.keras.models.Model(inputs=inputs, outputs=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3e4e8f80ca55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mX_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0moutput_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0moutput_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tf_explain/core/smoothgrad.py\u001b[0m in \u001b[0;36mexplain\u001b[0;34m(self, validation_data, model, class_index, num_samples, noise)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         grayscale_gradients = SmoothGrad.transform_to_grayscale(\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0msmoothed_gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         ).numpy()\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrepresentable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \"\"\"\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tf_explain.core.smoothgrad import SmoothGrad\n",
    "import pickle\n",
    "\n",
    "explainer = SmoothGrad()\n",
    "output_grid = {}\n",
    "output_n = {}\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  print(counter)\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    X_i = X[i]\n",
    "    X_i = tf.expand_dims(X_i, axis=0)\n",
    "    y_i = y_list[i]\n",
    "    grid = explainer.explain((X_i, _), mm, y_i, 20, 1.)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([output_grid, output_n], open( \"smoothgrad_female_all.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_grid, output_n = pickle.load(open( \"smoothgrad_female.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size):\n",
    "    \"\"\"\n",
    "    Replace a part of the image with a grey patch.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image\n",
    "        top_left_x (int): Top Left X position of the applied box\n",
    "        top_left_y (int): Top Left Y position of the applied box\n",
    "        patch_size (int): Size of patch to apply\n",
    "    Returns:\n",
    "        numpy.ndarray: Patched image\n",
    "    \"\"\"\n",
    "    patched_image = np.array(image, copy=True)\n",
    "    patched_image[\n",
    "        top_left_x : top_left_x + patch_size, top_left_y : top_left_y + patch_size, top_left_z : top_left_z + patch_size, :\n",
    "    ] = 0\n",
    "\n",
    "    return patched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sensgrid(image, mm, class_index, patch_size):\n",
    "  sensitivity_map = np.zeros((\n",
    "    math.ceil(image.shape[0] / patch_size),\n",
    "    math.ceil(image.shape[1] / patch_size),\n",
    "    math.ceil(image.shape[2] / patch_size)\n",
    "  ))\n",
    "  for index_z, top_left_z in enumerate(range(0, image.shape[2], patch_size)):\n",
    "    patches = [\n",
    "              apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size)\n",
    "              for index_x, top_left_x in enumerate(range(0, image.shape[0], patch_size))\n",
    "              for index_y, top_left_y in enumerate(range(0, image.shape[1], patch_size))\n",
    "              ]\n",
    "    coordinates = [\n",
    "                (index_y, index_x)\n",
    "                for index_x, _ in enumerate(range(0, image.shape[0], patch_size))\n",
    "                for index_y, _ in enumerate(range(0, image.shape[1], patch_size))\n",
    "                ]\n",
    "    predictions = mm.predict(np.array(patches), batch_size=1)\n",
    "    target_class_predictions = [prediction[class_index] for prediction in predictions]\n",
    "    for (index_y, index_x), confidence in zip(coordinates, target_class_predictions):\n",
    "      sensitivity_map[index_y, index_x, index_z] = 1 - confidence\n",
    "  sm = resize(sensitivity_map, (64,64,64))\n",
    "  heatmap = (sm - np.min(sm)) / (sm.max() - sm.min())\n",
    "  return(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'resize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-24ce84852ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mX_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0my_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sensgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0moutput_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0moutput_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-a314f14433ab>\u001b[0m in \u001b[0;36mget_sensgrid\u001b[0;34m(image, mm, class_index, patch_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0msensitivity_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_z\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0msm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensitivity_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m   \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'resize' is not defined"
     ]
    }
   ],
   "source": [
    "output_grid = {}\n",
    "output_n = {}\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  print(counter)\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    print(i)\n",
    "    X_i = X[i]\n",
    "    y_i = y_list[i]\n",
    "    grid = get_sensgrid(X_i, mm, y_i, 4)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1\n",
    "  if counter==6:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump([output_grid, output_n], open( \"heatmap_female_all.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 2.694525,
     "end_time": "2019-11-15T06:45:40.134006",
     "exception": true,
     "start_time": "2019-11-15T06:45:37.439481",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ded28046d863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "t2 = (batch['t2']-t2_mean)/t2_std\n",
    "ad = batch['ad']\n",
    "ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "ad = (ad-ad_mean)/ad_std\n",
    "fa = batch['fa']\n",
    "fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "fa = (fa-fa_mean)/fa_std\n",
    "md = batch['md']\n",
    "md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "md = (md-md_mean)/md_std\n",
    "rd = batch['rd']\n",
    "rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "rd = (rd-rd_mean)/rd_std\n",
    "#subjectid = decoder(batch['subjectid'])\n",
    "#y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "#X = tf.concat([t1, t2, ad, fa, md, rd], axis=4)\n",
    "X = tf.concat([t1, t2], axis=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(False)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.non_trainable_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "06_SimpleDL_MultiTask_Valid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "papermill": {
   "duration": 21323.320972,
   "end_time": "2019-11-15T06:45:47.268115",
   "environment_variables": {},
   "exception": true,
   "input_path": "RunModels_bck_densenet_t1t2.ipynb",
   "output_path": "RunModels_bck_densenet_t1t2_out.ipynb",
   "parameters": {},
   "start_time": "2019-11-15T00:50:23.947143",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
