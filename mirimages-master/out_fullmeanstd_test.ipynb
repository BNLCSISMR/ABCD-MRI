{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.056908,
     "end_time": "2019-11-18T12:57:07.058045",
     "exception": false,
     "start_time": "2019-11-18T12:57:07.001137",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48054,
     "status": "ok",
     "timestamp": 1573636583219,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "2mU_ZLUH5jz4",
    "outputId": "c4cbc445-56dd-49df-de87-c34e18da5f6c",
    "papermill": {
     "duration": 11.62569,
     "end_time": "2019-11-18T12:57:18.709075",
     "exception": false,
     "start_time": "2019-11-18T12:57:07.083385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7f8892f7b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7f8892f7b730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7f8798247ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7f8798247ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7f87982288c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7f87982288c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7f87981672f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7f87981672f0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7f87980a08c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7f87980a08c8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "from mricode.utils import log_textfile, createPath, data_generator\n",
    "from mricode.utils import copy_colab\n",
    "from mricode.utils import return_iter\n",
    "from mricode.utils import return_csv\n",
    "from mricode.config import config\n",
    "\n",
    "from mricode.models.SimpleCNN_small import SimpleCNN\n",
    "from mricode.models.DenseNet_NoDict import MyDenseNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow import nn\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras.engine.base_layer import InputSpec\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.081354,
     "end_time": "2019-11-18T12:57:18.817255",
     "exception": false,
     "start_time": "2019-11-18T12:57:18.735901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nH4XzW8C5yhH",
    "papermill": {
     "duration": 0.079085,
     "end_time": "2019-11-18T12:57:18.924195",
     "exception": false,
     "start_time": "2019-11-18T12:57:18.845110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_output = './output/'\n",
    "path_tfrecords = '/data2/res64/down/'\n",
    "path_csv = '/data2/csv/'\n",
    "filename_res = {'train': 'intell_residual_train.csv', 'val': 'intell_residual_valid.csv', 'test': 'intell_residual_test.csv'}\n",
    "filename_final = filename_res\n",
    "sample_size = 'site16_allimages'\n",
    "batch_size = 8\n",
    "onlyt1 = False\n",
    "modelname = 'densenet_allimages64'\n",
    "Model = SimpleCNN\n",
    "#Model = MyDenseNet\n",
    "t1_mean=1.3779395849814497\n",
    "t1_std=3.4895845243139503\n",
    "t2_mean=2.22435586968901\n",
    "t2_std=5.07708743178319\n",
    "ad_mean=1.3008901218593748e-05\n",
    "ad_std=0.009966655860940228\n",
    "fa_mean=0.0037552628409334037\n",
    "fa_std=0.012922319568740915\n",
    "md_mean=9.827903909139596e-06\n",
    "md_std=0.009956973204022659\n",
    "rd_mean=8.237404999587111e-06\n",
    "rd_std=0.009954672598675338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.350558,
     "end_time": "2019-11-18T12:57:19.300966",
     "exception": false,
     "start_time": "2019-11-18T12:57:18.950408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, norm_dict = return_csv(path_csv, filename_final, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.080078,
     "end_time": "2019-11-18T12:57:19.409562",
     "exception": false,
     "start_time": "2019-11-18T12:57:19.329484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "versionkey = 'cropped128'\n",
    "train_iter = config[versionkey]['iter_train']\n",
    "val_iter = config[versionkey]['iter_val']\n",
    "test_iter = config[versionkey]['iter_test']\n",
    "t1_mean = config[versionkey]['norm']['t1'][0]\n",
    "t1_std= config[versionkey]['norm']['t1'][1]\n",
    "t2_mean=config[versionkey]['norm']['t2'][0]\n",
    "t2_std=config[versionkey]['norm']['t2'][1]\n",
    "ad_mean=config[versionkey]['norm']['ad'][0]\n",
    "ad_std=config[versionkey]['norm']['ad'][1]\n",
    "fa_mean=config[versionkey]['norm']['fa'][0]\n",
    "fa_std=config[versionkey]['norm']['fa'][1]\n",
    "md_mean=config[versionkey]['norm']['md'][0]\n",
    "md_std=config[versionkey]['norm']['md'][1]\n",
    "rd_mean=config[versionkey]['norm']['rd'][0]\n",
    "rd_std=config[versionkey]['norm']['rd'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0058205845 1.0090235 -0.08604767 0.9554573\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-48ee507b2653>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_nan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mad\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mad_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mad_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ad'"
     ]
    }
   ],
   "source": [
    "t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "t2 = (batch['t2']-t2_mean)/t2_std\n",
    "print(np.mean(t1), np.std(t1), np.mean(t2), np.std(t2))\n",
    "if True:\n",
    "    ad = batch['ad']\n",
    "    ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "    ad = (ad-ad_mean)/ad_std\n",
    "    fa = batch['fa']\n",
    "    fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "    fa = (fa-fa_mean)/fa_std\n",
    "    md = batch['md']\n",
    "    md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "    md = (md-md_mean)/md_std\n",
    "    rd = batch['rd']\n",
    "    rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "    rd = (rd-rd_mean)/rd_std\n",
    "    print(np.mean(ad), np.std(ad), np.mean(fa), np.std(fa))\n",
    "    print(np.mean(md), np.std(md), np.mean(rd), np.std(rd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 12347.21977,
     "end_time": "2019-11-18T16:23:06.658321",
     "exception": false,
     "start_time": "2019-11-18T12:57:19.438551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py:193: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = asanyarray(arr - arrmean)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.81284585595131, 30.288950321353788, nan, nan)\n",
      "1.0\n",
      "(10.43483892083168, 29.85030963785138, nan, nan)\n",
      "2.0\n",
      "(10.402180661757788, 29.125928471280588, nan, nan)\n",
      "3.0\n",
      "(10.879297412931919, 28.874293025921894, nan, nan)\n",
      "4.0\n",
      "(10.925156563520432, 29.01586060587732, nan, nan)\n",
      "5.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d87998fe4368>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0mlog_textfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meanstcr256_new.log'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m       \u001b[0mt1_mean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mt1_std\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mmean\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3256\u001b[0m     return _methods._mean(a, axis=axis, dtype=dtype,\n\u001b[0;32m-> 3257\u001b[0;31m                           out=out, **kwargs)\n\u001b[0m\u001b[1;32m   3258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if True:\n",
    "  t1_mean = 0.\n",
    "  t1_std = 0.\n",
    "  t2_mean = 0.\n",
    "  t2_std = 0.\n",
    "  n = 0.\n",
    "  for b in train_iter:\n",
    "      log_textfile('meanstcr256_new.log',str(n))\n",
    "      t1_mean += np.mean(b['t1'])\n",
    "      t1_std += np.std(b['t1'])\n",
    "      t2 = b['t2']\n",
    "      t2_mean += np.mean(t2)\n",
    "      t2_std += np.std(t2)\n",
    "      n += 1\n",
    "      log_textfile('meanstcr256_new.log',str((t1_mean/n, t1_std/n, t2_mean/n, t2_std/n)))\n",
    "\n",
    "  t1_mean /= n\n",
    "  t1_std /= n\n",
    "  t2_mean /= n\n",
    "  t2_std /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = np.asarray(b['t2'], dtype=np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.reduce_mean(b['t1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(b['t2'])\n",
    "a = a.copy()\n",
    "a[np.isnan(a)] = 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "#decoder(batch['subjectid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([8, 256, 256, 256, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['t2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[7,128,128,128,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cast] name: Cast/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ec35976b2139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt1_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt2_mean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mt2_std\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    702\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   2210\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2213\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2214\u001b[0m   \u001b[0mDstT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDstT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DstT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[7,128,128,128,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cast] name: Cast/"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.541827,
     "end_time": "2019-11-18T16:23:10.142226",
     "exception": false,
     "start_time": "2019-11-18T16:23:09.600399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.6431085066123967, 3.774774262557981, 2.3879970019972974, 5.233771497359756)\n"
     ]
    }
   ],
   "source": [
    "log_textfile('meanstd256.log',str((t1_mean, t1_std, t2_mean, t2_std)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fdcadfa5ae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fdcadfa5ae8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fdcadf05400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fdcadf05400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fdcadf05ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fdcadf05ea0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fdcadf96a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fdcadf96a60> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fdc103d7950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fdc103d7950> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "4.0\n",
      "5.0\n",
      "6.0\n",
      "7.0\n",
      "8.0\n",
      "9.0\n",
      "10.0\n",
      "11.0\n",
      "12.0\n",
      "13.0\n",
      "14.0\n",
      "15.0\n",
      "16.0\n",
      "17.0\n",
      "18.0\n",
      "19.0\n",
      "20.0\n",
      "21.0\n",
      "22.0\n",
      "23.0\n",
      "24.0\n",
      "25.0\n",
      "26.0\n",
      "27.0\n",
      "28.0\n",
      "29.0\n",
      "30.0\n",
      "31.0\n",
      "32.0\n",
      "33.0\n",
      "34.0\n",
      "35.0\n",
      "36.0\n",
      "37.0\n",
      "38.0\n",
      "39.0\n",
      "40.0\n",
      "41.0\n",
      "42.0\n",
      "43.0\n",
      "44.0\n",
      "45.0\n",
      "46.0\n",
      "47.0\n",
      "48.0\n",
      "49.0\n",
      "50.0\n",
      "51.0\n",
      "52.0\n",
      "53.0\n",
      "54.0\n",
      "55.0\n",
      "56.0\n",
      "57.0\n",
      "58.0\n",
      "59.0\n",
      "60.0\n",
      "61.0\n",
      "62.0\n",
      "63.0\n",
      "64.0\n",
      "65.0\n",
      "66.0\n",
      "67.0\n",
      "68.0\n",
      "69.0\n",
      "70.0\n",
      "71.0\n",
      "72.0\n",
      "73.0\n",
      "74.0\n",
      "75.0\n",
      "76.0\n",
      "77.0\n",
      "78.0\n",
      "79.0\n",
      "80.0\n",
      "81.0\n",
      "82.0\n",
      "83.0\n",
      "84.0\n",
      "85.0\n",
      "86.0\n",
      "87.0\n",
      "88.0\n",
      "89.0\n",
      "90.0\n",
      "91.0\n",
      "92.0\n",
      "93.0\n",
      "94.0\n",
      "95.0\n",
      "96.0\n",
      "97.0\n",
      "98.0\n",
      "99.0\n",
      "100.0\n",
      "101.0\n",
      "102.0\n",
      "103.0\n",
      "104.0\n",
      "105.0\n",
      "106.0\n",
      "107.0\n",
      "108.0\n",
      "109.0\n",
      "110.0\n",
      "111.0\n",
      "112.0\n",
      "113.0\n",
      "114.0\n",
      "115.0\n",
      "116.0\n",
      "117.0\n",
      "118.0\n",
      "119.0\n",
      "120.0\n",
      "121.0\n",
      "122.0\n",
      "123.0\n",
      "124.0\n",
      "125.0\n",
      "126.0\n",
      "127.0\n",
      "128.0\n",
      "129.0\n",
      "130.0\n",
      "131.0\n",
      "132.0\n",
      "133.0\n",
      "134.0\n",
      "135.0\n",
      "136.0\n",
      "137.0\n",
      "138.0\n",
      "139.0\n",
      "140.0\n",
      "141.0\n",
      "142.0\n",
      "143.0\n",
      "144.0\n",
      "145.0\n",
      "146.0\n",
      "147.0\n",
      "148.0\n",
      "149.0\n",
      "150.0\n",
      "151.0\n",
      "152.0\n",
      "153.0\n",
      "154.0\n",
      "155.0\n",
      "156.0\n",
      "157.0\n",
      "158.0\n",
      "159.0\n",
      "160.0\n",
      "161.0\n",
      "162.0\n",
      "163.0\n",
      "164.0\n",
      "165.0\n",
      "166.0\n",
      "167.0\n",
      "168.0\n",
      "169.0\n",
      "170.0\n",
      "171.0\n",
      "172.0\n",
      "173.0\n",
      "174.0\n",
      "175.0\n",
      "176.0\n",
      "177.0\n",
      "178.0\n",
      "179.0\n",
      "180.0\n",
      "181.0\n",
      "182.0\n",
      "183.0\n",
      "184.0\n",
      "185.0\n",
      "186.0\n",
      "187.0\n",
      "188.0\n",
      "189.0\n",
      "190.0\n",
      "191.0\n",
      "192.0\n",
      "193.0\n",
      "194.0\n",
      "195.0\n",
      "196.0\n",
      "197.0\n",
      "198.0\n",
      "199.0\n",
      "200.0\n",
      "201.0\n",
      "202.0\n",
      "203.0\n",
      "204.0\n",
      "205.0\n",
      "206.0\n",
      "207.0\n",
      "208.0\n",
      "209.0\n",
      "210.0\n",
      "211.0\n",
      "212.0\n",
      "213.0\n",
      "214.0\n",
      "215.0\n",
      "216.0\n",
      "217.0\n",
      "218.0\n",
      "219.0\n",
      "220.0\n",
      "221.0\n",
      "222.0\n",
      "223.0\n",
      "224.0\n",
      "225.0\n",
      "226.0\n",
      "227.0\n",
      "228.0\n",
      "229.0\n",
      "230.0\n",
      "231.0\n",
      "232.0\n",
      "233.0\n",
      "234.0\n",
      "235.0\n",
      "236.0\n",
      "237.0\n",
      "238.0\n",
      "239.0\n",
      "240.0\n",
      "241.0\n",
      "242.0\n",
      "243.0\n",
      "244.0\n",
      "245.0\n",
      "246.0\n",
      "247.0\n",
      "248.0\n",
      "249.0\n",
      "250.0\n",
      "251.0\n",
      "252.0\n",
      "253.0\n",
      "254.0\n",
      "255.0\n",
      "256.0\n",
      "257.0\n",
      "258.0\n",
      "259.0\n",
      "260.0\n",
      "261.0\n",
      "262.0\n",
      "263.0\n",
      "264.0\n",
      "265.0\n",
      "266.0\n",
      "267.0\n",
      "268.0\n",
      "269.0\n",
      "270.0\n",
      "271.0\n",
      "272.0\n",
      "273.0\n",
      "274.0\n",
      "275.0\n",
      "276.0\n",
      "277.0\n",
      "278.0\n",
      "279.0\n",
      "280.0\n",
      "281.0\n",
      "282.0\n",
      "283.0\n",
      "284.0\n",
      "285.0\n",
      "286.0\n",
      "287.0\n",
      "288.0\n",
      "289.0\n",
      "290.0\n",
      "291.0\n",
      "292.0\n",
      "293.0\n",
      "294.0\n",
      "295.0\n",
      "296.0\n",
      "297.0\n",
      "298.0\n",
      "299.0\n",
      "300.0\n",
      "301.0\n",
      "302.0\n",
      "303.0\n",
      "304.0\n",
      "305.0\n",
      "306.0\n",
      "307.0\n",
      "308.0\n",
      "309.0\n",
      "310.0\n",
      "311.0\n",
      "312.0\n",
      "313.0\n",
      "314.0\n",
      "315.0\n",
      "316.0\n",
      "317.0\n",
      "318.0\n",
      "319.0\n",
      "320.0\n",
      "321.0\n",
      "322.0\n",
      "323.0\n",
      "324.0\n",
      "325.0\n",
      "326.0\n",
      "327.0\n",
      "328.0\n",
      "329.0\n",
      "330.0\n",
      "331.0\n",
      "332.0\n",
      "333.0\n",
      "334.0\n",
      "335.0\n",
      "336.0\n",
      "337.0\n",
      "338.0\n",
      "339.0\n",
      "340.0\n",
      "341.0\n",
      "342.0\n",
      "343.0\n",
      "344.0\n",
      "345.0\n",
      "346.0\n",
      "347.0\n",
      "348.0\n",
      "349.0\n",
      "350.0\n",
      "351.0\n",
      "352.0\n",
      "353.0\n",
      "354.0\n",
      "355.0\n",
      "356.0\n",
      "357.0\n",
      "358.0\n",
      "359.0\n",
      "360.0\n",
      "361.0\n",
      "362.0\n",
      "363.0\n",
      "364.0\n",
      "365.0\n",
      "366.0\n",
      "367.0\n",
      "368.0\n",
      "369.0\n",
      "370.0\n",
      "371.0\n",
      "372.0\n",
      "373.0\n",
      "374.0\n",
      "375.0\n",
      "376.0\n",
      "377.0\n",
      "378.0\n",
      "379.0\n",
      "380.0\n",
      "381.0\n",
      "382.0\n",
      "383.0\n",
      "384.0\n",
      "385.0\n",
      "386.0\n",
      "387.0\n",
      "388.0\n",
      "389.0\n",
      "390.0\n",
      "391.0\n",
      "392.0\n",
      "393.0\n",
      "394.0\n",
      "395.0\n",
      "396.0\n",
      "397.0\n",
      "398.0\n",
      "399.0\n",
      "400.0\n",
      "401.0\n",
      "402.0\n",
      "403.0\n",
      "404.0\n",
      "405.0\n",
      "406.0\n",
      "407.0\n",
      "408.0\n",
      "409.0\n",
      "410.0\n",
      "411.0\n",
      "412.0\n",
      "413.0\n",
      "414.0\n",
      "415.0\n",
      "416.0\n",
      "417.0\n",
      "418.0\n",
      "419.0\n",
      "420.0\n",
      "421.0\n",
      "422.0\n",
      "423.0\n",
      "424.0\n",
      "425.0\n",
      "426.0\n",
      "427.0\n",
      "428.0\n",
      "429.0\n",
      "430.0\n",
      "431.0\n",
      "432.0\n",
      "433.0\n",
      "434.0\n",
      "435.0\n",
      "436.0\n",
      "437.0\n",
      "438.0\n",
      "439.0\n",
      "440.0\n",
      "441.0\n",
      "442.0\n",
      "443.0\n",
      "444.0\n",
      "445.0\n",
      "446.0\n",
      "447.0\n",
      "448.0\n",
      "449.0\n",
      "450.0\n",
      "451.0\n",
      "452.0\n",
      "453.0\n",
      "454.0\n",
      "455.0\n",
      "456.0\n",
      "457.0\n",
      "458.0\n",
      "459.0\n",
      "460.0\n",
      "461.0\n",
      "462.0\n",
      "463.0\n",
      "464.0\n",
      "465.0\n",
      "466.0\n",
      "467.0\n",
      "468.0\n",
      "469.0\n",
      "470.0\n",
      "471.0\n",
      "472.0\n",
      "473.0\n",
      "474.0\n",
      "475.0\n",
      "476.0\n",
      "477.0\n",
      "478.0\n",
      "479.0\n",
      "480.0\n",
      "481.0\n",
      "482.0\n",
      "483.0\n",
      "484.0\n",
      "485.0\n",
      "486.0\n",
      "487.0\n",
      "488.0\n",
      "489.0\n",
      "490.0\n",
      "491.0\n",
      "492.0\n",
      "493.0\n",
      "494.0\n",
      "495.0\n",
      "496.0\n",
      "497.0\n",
      "498.0\n",
      "499.0\n",
      "500.0\n",
      "501.0\n",
      "502.0\n",
      "503.0\n",
      "504.0\n",
      "505.0\n",
      "506.0\n",
      "507.0\n",
      "508.0\n",
      "509.0\n",
      "510.0\n",
      "511.0\n",
      "512.0\n",
      "513.0\n",
      "514.0\n",
      "515.0\n",
      "516.0\n",
      "517.0\n",
      "518.0\n",
      "519.0\n",
      "520.0\n",
      "521.0\n",
      "522.0\n",
      "523.0\n",
      "524.0\n",
      "525.0\n",
      "526.0\n",
      "527.0\n",
      "528.0\n",
      "529.0\n",
      "530.0\n",
      "531.0\n",
      "532.0\n",
      "533.0\n",
      "534.0\n",
      "535.0\n",
      "536.0\n",
      "537.0\n",
      "538.0\n",
      "539.0\n",
      "540.0\n",
      "541.0\n",
      "542.0\n",
      "543.0\n",
      "544.0\n",
      "545.0\n",
      "546.0\n",
      "547.0\n",
      "548.0\n",
      "549.0\n",
      "550.0\n",
      "551.0\n",
      "552.0\n",
      "553.0\n",
      "554.0\n",
      "555.0\n",
      "556.0\n",
      "557.0\n",
      "558.0\n",
      "559.0\n",
      "560.0\n",
      "561.0\n",
      "562.0\n",
      "563.0\n",
      "564.0\n",
      "565.0\n",
      "566.0\n",
      "567.0\n",
      "568.0\n",
      "569.0\n",
      "570.0\n",
      "571.0\n",
      "572.0\n",
      "573.0\n",
      "574.0\n",
      "575.0\n",
      "576.0\n",
      "577.0\n",
      "578.0\n",
      "579.0\n",
      "580.0\n",
      "581.0\n",
      "582.0\n",
      "583.0\n",
      "584.0\n",
      "585.0\n",
      "586.0\n",
      "587.0\n",
      "588.0\n",
      "589.0\n",
      "590.0\n",
      "591.0\n",
      "592.0\n",
      "593.0\n",
      "594.0\n",
      "595.0\n",
      "596.0\n",
      "597.0\n",
      "598.0\n",
      "599.0\n",
      "600.0\n",
      "601.0\n",
      "602.0\n",
      "603.0\n",
      "604.0\n",
      "605.0\n",
      "606.0\n",
      "607.0\n",
      "608.0\n",
      "609.0\n",
      "610.0\n",
      "611.0\n",
      "612.0\n",
      "613.0\n",
      "614.0\n",
      "615.0\n",
      "616.0\n",
      "617.0\n",
      "618.0\n",
      "619.0\n",
      "620.0\n",
      "621.0\n",
      "622.0\n",
      "623.0\n",
      "624.0\n",
      "625.0\n",
      "626.0\n",
      "627.0\n",
      "628.0\n",
      "629.0\n",
      "630.0\n",
      "631.0\n",
      "632.0\n",
      "633.0\n",
      "634.0\n",
      "635.0\n",
      "636.0\n",
      "637.0\n",
      "638.0\n",
      "639.0\n",
      "640.0\n",
      "641.0\n",
      "642.0\n",
      "643.0\n",
      "644.0\n",
      "645.0\n",
      "646.0\n",
      "647.0\n",
      "648.0\n",
      "649.0\n",
      "650.0\n",
      "651.0\n",
      "652.0\n",
      "653.0\n",
      "654.0\n",
      "655.0\n",
      "656.0\n",
      "657.0\n",
      "658.0\n",
      "659.0\n",
      "660.0\n",
      "661.0\n",
      "662.0\n",
      "663.0\n",
      "664.0\n",
      "665.0\n",
      "666.0\n",
      "667.0\n",
      "668.0\n",
      "669.0\n",
      "670.0\n",
      "671.0\n",
      "672.0\n",
      "673.0\n",
      "674.0\n",
      "675.0\n",
      "676.0\n",
      "677.0\n",
      "678.0\n",
      "679.0\n",
      "680.0\n",
      "681.0\n",
      "682.0\n",
      "683.0\n",
      "684.0\n",
      "685.0\n",
      "686.0\n",
      "687.0\n",
      "688.0\n",
      "689.0\n",
      "690.0\n",
      "691.0\n",
      "692.0\n",
      "693.0\n",
      "694.0\n",
      "695.0\n",
      "696.0\n",
      "697.0\n",
      "698.0\n",
      "699.0\n",
      "700.0\n",
      "701.0\n",
      "702.0\n",
      "703.0\n",
      "704.0\n",
      "705.0\n",
      "706.0\n",
      "707.0\n",
      "708.0\n",
      "709.0\n",
      "710.0\n",
      "711.0\n",
      "712.0\n",
      "713.0\n",
      "714.0\n",
      "715.0\n",
      "716.0\n",
      "717.0\n",
      "718.0\n",
      "719.0\n",
      "720.0\n",
      "721.0\n",
      "722.0\n",
      "723.0\n",
      "724.0\n",
      "725.0\n",
      "726.0\n",
      "727.0\n",
      "728.0\n"
     ]
    }
   ],
   "source": [
    "train_iter = config['cropped64']['iter_train']\n",
    "val_iter = config['cropped64']['iter_val']\n",
    "test_iter = config['cropped64']['iter_test']\n",
    "if True:\n",
    "  t1_mean = 0.\n",
    "  t1_std = 0.\n",
    "  t2_mean = 0.\n",
    "  t2_std = 0.\n",
    "  ad_mean = 0.\n",
    "  ad_std = 0.\n",
    "  fa_mean = 0.\n",
    "  fa_std = 0.\n",
    "  md_mean = 0.\n",
    "  md_std = 0.\n",
    "  rd_mean = 0.\n",
    "  rd_std = 0.\n",
    "  n = 0.\n",
    "  for b in train_iter:\n",
    "      print(n)\n",
    "      t1_mean += np.mean(b['t1'])\n",
    "      t1_std += np.std(b['t1'])\n",
    "      t2_mean += np.mean(b['t2'])\n",
    "      t2_std += np.std(b['t2'])\n",
    "      a = np.asarray(b['ad'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      ad_mean += np.mean(a)\n",
    "      ad_std += np.std(a)\n",
    "      a = np.asarray(b['fa'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      fa_mean += np.mean(a)\n",
    "      fa_std += np.std(a)\n",
    "      a = np.asarray(b['md'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      md_mean += np.mean(a)\n",
    "      md_std += np.std(a)\n",
    "      a = np.asarray(b['rd'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      rd_mean += np.mean(a)\n",
    "      rd_std += np.std(a)\n",
    "      n += 1\n",
    "  t1_mean /= n\n",
    "  t1_std /= n\n",
    "  t2_mean /= n\n",
    "  t2_std /= n\n",
    "  ad_mean /= n\n",
    "  ad_std /= n\n",
    "  fa_mean /= n\n",
    "  fa_std /= n\n",
    "  md_mean /= n\n",
    "  md_std /= n\n",
    "  rd_mean /= n\n",
    "  rd_std /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJ0VnyEzBAes",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2924,
     "status": "ok",
     "timestamp": 1573636640716,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "mSwhEqDPM_r8",
    "outputId": "14a4ac0b-9587-4776-fc64-1e008f0c5c31",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11312570418978232,\n",
       " 0.14135518265387828,\n",
       " 27.98422794577516,\n",
       " 45.90435212793337,\n",
       " 0.0007140914352312044,\n",
       " 0.1471820359293305,\n",
       " 0.18089792297946083,\n",
       " 0.18397762627431558,\n",
       " 0.0005650996822530141,\n",
       " 0.14559568945663873,\n",
       " 0.0004971999132421456,\n",
       " 0.13345626959299658)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1_mean, t1_std, t2_mean, t2_std, ad_mean, ad_std, fa_mean, fa_std, md_mean, md_std, rd_mean, rd_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAyV5ktlA6K1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_files = glob.glob('/data/home/benedikt_d_schifferer/data_T2_lowerres_cropped128/' + '*.gz') \n",
    "all_files += glob.glob('/data/home/benedikt_d_schifferer/data_T1_lowerres_cropped128/' + '*.gz')\n",
    "all_files = glob.glob('/data/home/benedikt_d_schifferer/data_T1_T2_201909/' + '*.gz') \n",
    "\n",
    "train_iter = data_generator(list(train_df['subjectkey'].values), all_files, batch_size, \n",
    "                            input_shape=(256,256,256))\n",
    "val_iter = data_generator(list(val_df['subjectkey'].values), all_files, batch_size, \n",
    "                            is_validation_data=True, \n",
    "                            input_shape=(256,256,256))\n",
    "test_iter = data_generator(list(test_df['subjectkey'].values), all_files, batch_size, \n",
    "                            is_validation_data=True, \n",
    "                            input_shape=(256,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "  t1_mean = 0.\n",
    "  t1_std = 0.\n",
    "  t2_mean = 0.\n",
    "  t2_std = 0.\n",
    "  n = 0.\n",
    "  for b in train_iter:\n",
    "      t1_mean += np.mean(b['t1'])\n",
    "      t1_std += np.std(b['t1'])\n",
    "      t2_mean += np.mean(b['t2'])\n",
    "      t2_std += np.std(b['t2'])\n",
    "      n += np.asarray(b['t1']).shape[0]\n",
    "      print(n)\n",
    "\n",
    "  t1_mean /= n\n",
    "  t1_std /= n\n",
    "  t2_mean /= n\n",
    "  t2_std /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3147,
     "status": "ok",
     "timestamp": 1573636641787,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "0xYy4XUyVBeC",
    "outputId": "a6a19d59-31ff-4064-f1dc-952befcce4b3",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTk95ptFV5oN",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = {'female': 2, 'race.ethnicity': 5, 'high.educ_group': 4, 'income_group': 8, 'married': 6}\n",
    "num_cols = [x for x in list(val_df.columns) if '_norm' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hROMApYiDagm",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict):\n",
    "  for col in num_cols:\n",
    "    tmp_col = col\n",
    "    tmp_std = norm_dict[tmp_col.replace('_norm','')]['std']\n",
    "    tmp_y_true = tf.cast(y_true[col], tf.float32).numpy()\n",
    "    tmp_y_pred = np.squeeze(y_pred[col].numpy())\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    else:\n",
    "      out_loss[tmp_col] += np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "    else:\n",
    "      out_acc[tmp_col] += np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "  for col in list(cat_cols.keys()):\n",
    "    tmp_col = col\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    else:\n",
    "      out_loss[tmp_col] += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()\n",
    "    else:\n",
    "      out_acc[tmp_col] += tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()    \n",
    "  return(out_loss, out_acc)\n",
    "\n",
    "def format_output(out_loss, out_acc, n, cols, print_bl=False):\n",
    "  loss = 0\n",
    "  acc = 0\n",
    "  output = []\n",
    "  for col in cols:\n",
    "    output.append([col, out_loss[col]/n, out_acc[col]/n])\n",
    "    loss += out_loss[col]/n\n",
    "    acc += out_acc[col]/n\n",
    "  df = pd.DataFrame(output)\n",
    "  df.columns = ['name', 'loss', 'acc']\n",
    "  if print_bl:\n",
    "    print(df)\n",
    "  return(loss, acc, df)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, y, model, optimizer, cat_cols, num_cols):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(X)\n",
    "    i = 0\n",
    "    loss = tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for i in range(1,len(num_cols)):\n",
    "      loss += tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for col in list(cat_cols.keys()):\n",
    "      loss += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y[col]), tf.squeeze(predictions[col]))\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]\n",
    "  with tf.control_dependencies(mean_std):\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return(y, predictions, loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(X, y, model):\n",
    "  predictions = model(X)\n",
    "  return(y, predictions)\n",
    "\n",
    "def epoch(data_iter, df, model, optimizer, cat_cols, num_cols, norm_dict):\n",
    "  out_loss = {}\n",
    "  out_acc = {}\n",
    "  n = 0.\n",
    "  n_batch = 0.\n",
    "  total_time_dataload = 0.\n",
    "  total_time_model = 0.\n",
    "  start_time = time.time()\n",
    "  for batch in data_iter:\n",
    "    total_time_dataload += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "    t2 = (batch['t2']-t2_mean)/t2_std\n",
    "    if False:\n",
    "        ad = batch['ad']\n",
    "        ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "        ad = (ad-ad_mean)/ad_std\n",
    "        fa = batch['fa']\n",
    "        fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "        fa = (fa-fa_mean)/fa_std\n",
    "        md = batch['md']\n",
    "        md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "        md = (md-md_mean)/md_std\n",
    "        rd = batch['rd']\n",
    "        rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "        rd = (rd-rd_mean)/rd_std\n",
    "    subjectid = decoder(batch['subjectid'])\n",
    "    y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "    #X = tf.concat([t1], axis=4)\n",
    "    X = tf.concat([t1, t2], axis=4)\n",
    "    if optimizer != None:\n",
    "      y_true, y_pred, loss = train_step(X, y, model, optimizer, cat_cols, num_cols)\n",
    "    else:\n",
    "      y_true, y_pred = test_step(X, y, model)\n",
    "    out_loss, out_acc = calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict)\n",
    "    n += X.shape[0]\n",
    "    n_batch += 1\n",
    "    if (n_batch % 10) == 0:\n",
    "      print(n_batch)\n",
    "    total_time_model += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "  return (out_loss, out_acc, n, total_time_model, total_time_dataload)\n",
    "\n",
    "def get_labels(df, subjectid, cols = ['nihtbx_fluidcomp_uncorrected_norm']):\n",
    "  subjects_df = pd.DataFrame(subjectid)\n",
    "  result_df = pd.merge(subjects_df, df, left_on=0, right_on='subjectkey', how='left')\n",
    "  output = {}\n",
    "  for col in cols:\n",
    "    output[col] = np.asarray(result_df[col].values)\n",
    "  return output\n",
    "\n",
    "def best_val(df_best, df_val, df_test, e):\n",
    "  df_best = pd.merge(df_best, df_val, how='left', left_on='name', right_on='name')\n",
    "  df_best = pd.merge(df_best, df_test, how='left', left_on='name', right_on='name')\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_epochs'] = e\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_epochs'] = e\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_test'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_test']\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_val'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_val']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best = df_best.drop(['cur_loss_val', 'cur_acc_val', 'cur_loss_test', 'cur_acc_test'], axis=1)\n",
    "  return(df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46355,
     "status": "ok",
     "timestamp": 1573636701683,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "RV24LE3k-00n",
    "outputId": "067125be-6ce2-40c9-998e-62f800ccb2f1",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "template = 'Epoch {0}, Loss: {1:.3f}, Accuracy: {2:.3f}, Val Loss: {3:.3f}, Val Accuracy: {4:.3f}, Time Model: {5:.3f}, Time Data: {6:.3f}'\n",
    "for col in [0]:\n",
    "  log_textfile(path_output + modelname + '/log' + '.log', cat_cols),\n",
    "  log_textfile(path_output + modelname + '/log' + '.log', num_cols)\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "  model = Model(cat_cols, num_cols)\n",
    "  df_best = None\n",
    "  for e in range(20):\n",
    "    log_textfile(path_output + modelname + '/log' + '.log', 'Epochs: ' + str(e))\n",
    "    loss = tf.Variable(0.)\n",
    "    acc = tf.Variable(0.) \n",
    "    val_loss = tf.Variable(0.)\n",
    "    val_acc = tf.Variable(0.)\n",
    "    test_loss = tf.Variable(0.)\n",
    "    test_acc = tf.Variable(0.)\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "    train_out_loss, train_out_acc, n, time_model, time_data = epoch(train_iter, train_df, model, optimizer, cat_cols, num_cols, norm_dict)\n",
    "    tf.keras.backend.set_learning_phase(False)\n",
    "    val_out_loss, val_out_acc, n, _, _ = epoch(val_iter, val_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "    test_out_loss, test_out_acc, n, _, _ = epoch(test_iter, test_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "    loss, acc, _ = format_output(train_out_loss, train_out_acc, n, list(cat_cols.keys())+num_cols)\n",
    "    val_loss, val_acc, df_val = format_output(val_out_loss, val_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "    test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "    df_val.columns = ['name', 'cur_loss_val', 'cur_acc_val']\n",
    "    df_test.columns = ['name', 'cur_loss_test', 'cur_acc_test']\n",
    "    if e == 0:\n",
    "      df_best = pd.merge(df_test, df_val, how='left', left_on='name', right_on='name')\n",
    "      df_best['best_acc_epochs'] = 0\n",
    "      df_best['best_loss_epochs'] = 0\n",
    "      df_best.columns = ['name', 'best_loss_test', 'best_acc_test', 'best_loss_val', 'best_acc_val', 'best_acc_epochs', 'best_loss_epochs']\n",
    "    df_best = best_val(df_best, df_val, df_test, e)\n",
    "    print(df_best[['name', 'best_loss_test', 'best_acc_test']])\n",
    "    print(df_best[['name', 'best_loss_val', 'best_acc_val']])\n",
    "    log_textfile(path_output + modelname + '/log' + '.log', template.format(e, loss, acc, val_loss, val_acc, time_model, time_data))\n",
    "    if e in [10, 15]:\n",
    "      optimizer.lr = optimizer.lr/3\n",
    "      log_textfile(path_output + modelname + '/log' + '.log', 'Learning rate: ' + str(optimizer.lr))\n",
    "    df_best.to_csv(path_output +  modelname + '/df_best' + str(e) + '.csv')\n",
    "    df_best.to_csv(path_output +  modelname + '/df_best' + '.csv')\n",
    "    model.save_weights(path_output + modelname + '/checkpoints/' + str(e) + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('final_output_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(64,64,64,2), name='inputlayer123')\n",
    "a = model(inputs)['female']\n",
    "mm = tf.keras.models.Model(inputs=inputs, outputs=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_explain.core.smoothgrad import SmoothGrad\n",
    "import pickle\n",
    "\n",
    "explainer = SmoothGrad()\n",
    "output_grid = {}\n",
    "output_n = {}\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  print(counter)\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    X_i = X[i]\n",
    "    X_i = tf.expand_dims(X_i, axis=0)\n",
    "    y_i = y_list[i]\n",
    "    grid = explainer.explain((X_i, _), mm, y_i, 20, 1.)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump([output_grid, output_n], open( \"smoothgrad_female_all.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_grid, output_n = pickle.load(open( \"smoothgrad_female.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size):\n",
    "    \"\"\"\n",
    "    Replace a part of the image with a grey patch.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image\n",
    "        top_left_x (int): Top Left X position of the applied box\n",
    "        top_left_y (int): Top Left Y position of the applied box\n",
    "        patch_size (int): Size of patch to apply\n",
    "    Returns:\n",
    "        numpy.ndarray: Patched image\n",
    "    \"\"\"\n",
    "    patched_image = np.array(image, copy=True)\n",
    "    patched_image[\n",
    "        top_left_x : top_left_x + patch_size, top_left_y : top_left_y + patch_size, top_left_z : top_left_z + patch_size, :\n",
    "    ] = 0\n",
    "\n",
    "    return patched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sensgrid(image, mm, class_index, patch_size):\n",
    "  sensitivity_map = np.zeros((\n",
    "    math.ceil(image.shape[0] / patch_size),\n",
    "    math.ceil(image.shape[1] / patch_size),\n",
    "    math.ceil(image.shape[2] / patch_size)\n",
    "  ))\n",
    "  for index_z, top_left_z in enumerate(range(0, image.shape[2], patch_size)):\n",
    "    patches = [\n",
    "              apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size)\n",
    "              for index_x, top_left_x in enumerate(range(0, image.shape[0], patch_size))\n",
    "              for index_y, top_left_y in enumerate(range(0, image.shape[1], patch_size))\n",
    "              ]\n",
    "    coordinates = [\n",
    "                (index_y, index_x)\n",
    "                for index_x, _ in enumerate(range(0, image.shape[0], patch_size))\n",
    "                for index_y, _ in enumerate(range(0, image.shape[1], patch_size))\n",
    "                ]\n",
    "    predictions = mm.predict(np.array(patches), batch_size=1)\n",
    "    target_class_predictions = [prediction[class_index] for prediction in predictions]\n",
    "    for (index_y, index_x), confidence in zip(coordinates, target_class_predictions):\n",
    "      sensitivity_map[index_y, index_x, index_z] = 1 - confidence\n",
    "  sm = resize(sensitivity_map, (64,64,64))\n",
    "  heatmap = (sm - np.min(sm)) / (sm.max() - sm.min())\n",
    "  return(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_grid = {}\n",
    "output_n = {}\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  print(counter)\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    print(i)\n",
    "    X_i = X[i]\n",
    "    y_i = y_list[i]\n",
    "    grid = get_sensgrid(X_i, mm, y_i, 4)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1\n",
    "  if counter==6:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump([output_grid, output_n], open( \"heatmap_female_all.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "t2 = (batch['t2']-t2_mean)/t2_std\n",
    "ad = batch['ad']\n",
    "ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "ad = (ad-ad_mean)/ad_std\n",
    "fa = batch['fa']\n",
    "fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "fa = (fa-fa_mean)/fa_std\n",
    "md = batch['md']\n",
    "md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "md = (md-md_mean)/md_std\n",
    "rd = batch['rd']\n",
    "rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "rd = (rd-rd_mean)/rd_std\n",
    "#subjectid = decoder(batch['subjectid'])\n",
    "#y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "#X = tf.concat([t1, t2, ad, fa, md, rd], axis=4)\n",
    "X = tf.concat([t1, t2], axis=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(False)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.non_trainable_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "06_SimpleDL_MultiTask_Valid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "papermill": {
   "duration": 12367.276949,
   "end_time": "2019-11-18T16:23:13.320293",
   "environment_variables": {},
   "exception": true,
   "input_path": "RunModels.ipynb",
   "output_path": "out_fullmeanstd.ipynb",
   "parameters": {},
   "start_time": "2019-11-18T12:57:06.043344",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
