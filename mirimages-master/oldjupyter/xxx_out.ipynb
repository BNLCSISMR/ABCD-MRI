{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true,
    "inputHidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [26]'.</span>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at 'In [26]'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 0.051438,
     "end_time": "2019-11-24T00:48:35.362285",
     "exception": false,
     "start_time": "2019-11-24T00:48:35.310847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48054,
     "status": "ok",
     "timestamp": 1573636583219,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "2mU_ZLUH5jz4",
    "outputId": "c4cbc445-56dd-49df-de87-c34e18da5f6c",
    "papermill": {
     "duration": 36.887955,
     "end_time": "2019-11-24T00:49:12.279305",
     "exception": false,
     "start_time": "2019-11-24T00:48:35.391350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc744604730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc744604730> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc6902997b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc6902997b8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc690137268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc690137268> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc6902049d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc6902049d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc6900e5510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc6900e5510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.transform import resize\n",
    "\n",
    "from mricode.utils import log_textfile, createPath, data_generator\n",
    "from mricode.utils import copy_colab\n",
    "from mricode.utils import return_iter\n",
    "from mricode.utils import return_csv\n",
    "from mricode.config import config\n",
    "\n",
    "from mricode.models.SimpleCNN_normal import SimpleCNN\n",
    "from mricode.models.DenseNet_NoDict import MyDenseNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow import nn\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras.engine.base_layer import InputSpec\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 0.077501,
     "end_time": "2019-11-24T00:49:12.388699",
     "exception": false,
     "start_time": "2019-11-24T00:49:12.311198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nH4XzW8C5yhH",
    "papermill": {
     "duration": 0.072188,
     "end_time": "2019-11-24T00:49:12.491816",
     "exception": false,
     "start_time": "2019-11-24T00:49:12.419628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_output = './output/'\n",
    "path_tfrecords = '/data2/res64/down/'\n",
    "path_csv = '/data2/csv/'\n",
    "filename_res = {'train': 'intell_residual_train.csv', 'val': 'intell_residual_valid.csv', 'test': 'intell_residual_test.csv'}\n",
    "filename_final = filename_res\n",
    "sample_size = 'site16_allimages'\n",
    "batch_size = 8\n",
    "onlyt1 = False\n",
    "#Model = SimpleCNN\n",
    "Model = MyDenseNet\n",
    "versionkey = 'down64' #down256, cropped128, cropped64, down64\n",
    "modelname = 'new2_densenet_allimages_' + versionkey\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "papermill": {
     "duration": 0.073012,
     "end_time": "2019-11-24T00:49:12.596690",
     "exception": false,
     "start_time": "2019-11-24T00:49:12.523678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "createPath(path_output + modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "papermill": {
     "duration": 0.339959,
     "end_time": "2019-11-24T00:49:12.968231",
     "exception": false,
     "start_time": "2019-11-24T00:49:12.628272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, norm_dict = return_csv(path_csv, filename_final, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "papermill": {
     "duration": 0.077166,
     "end_time": "2019-11-24T00:49:13.077427",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.000261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_iter = config[versionkey]['iter_train']\n",
    "val_iter = config[versionkey]['iter_val']\n",
    "test_iter = config[versionkey]['iter_test']\n",
    "t1_mean = config[versionkey]['norm']['t1'][0]\n",
    "t1_std= config[versionkey]['norm']['t1'][1]\n",
    "t2_mean=config[versionkey]['norm']['t2'][0]\n",
    "t2_std=config[versionkey]['norm']['t2'][1]\n",
    "ad_mean=config[versionkey]['norm']['ad'][0]\n",
    "ad_std=config[versionkey]['norm']['ad'][1]\n",
    "fa_mean=config[versionkey]['norm']['fa'][0]\n",
    "fa_std=config[versionkey]['norm']['fa'][1]\n",
    "md_mean=config[versionkey]['norm']['md'][0]\n",
    "md_std=config[versionkey]['norm']['md'][1]\n",
    "rd_mean=config[versionkey]['norm']['rd'][0]\n",
    "rd_std=config[versionkey]['norm']['rd'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3147,
     "status": "ok",
     "timestamp": 1573636641787,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "0xYy4XUyVBeC",
    "outputId": "a6a19d59-31ff-4064-f1dc-952befcce4b3",
    "papermill": {
     "duration": 0.075493,
     "end_time": "2019-11-24T00:49:13.185000",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.109507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BMI': {'mean': 18.681548127052135, 'std': 4.193043131845343},\n",
       " 'age': {'mean': 119.00325844623563, 'std': 7.479129774017182},\n",
       " 'height': {'mean': 55.25173666322929, 'std': 3.152756181679028},\n",
       " 'nihtbx_cardsort_uncorrected': {'mean': 0.01902727147573316,\n",
       "  'std': 0.92710655806542},\n",
       " 'nihtbx_cryst_uncorrected': {'mean': 0.007018628014748754,\n",
       "  'std': 0.7845373584638602},\n",
       " 'nihtbx_flanker_uncorrected': {'mean': 0.02188780794049048,\n",
       "  'std': 0.9070917080607726},\n",
       " 'nihtbx_fluidcomp_uncorrected': {'mean': 0.020178243913565427,\n",
       "  'std': 0.86606123778624},\n",
       " 'nihtbx_list_uncorrected': {'mean': 0.00625176016120734,\n",
       "  'std': 0.8898550695735616},\n",
       " 'nihtbx_pattern_uncorrected': {'mean': 0.020721412569885096,\n",
       "  'std': 0.9486618556882954},\n",
       " 'nihtbx_picture_uncorrected': {'mean': 0.0005782175223803825,\n",
       "  'std': 0.9577989703304521},\n",
       " 'nihtbx_picvocab_uncorrected': {'mean': 0.0068509109986280275,\n",
       "  'std': 0.8038465951211212},\n",
       " 'nihtbx_reading_uncorrected': {'mean': 0.0027847502208883574,\n",
       "  'std': 0.8572703419965433},\n",
       " 'nihtbx_totalcomp_uncorrected': {'mean': 0.016971206631452577,\n",
       "  'std': 0.7866686549175077},\n",
       " 'vol': {'mean': 0.013939557398902416, 'std': 1.001699783217792},\n",
       " 'weight': {'mean': 81.91536668875837, 'std': 23.321845390211728}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTk95ptFV5oN",
    "papermill": {
     "duration": 0.071528,
     "end_time": "2019-11-24T00:49:13.288519",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.216991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cat_cols = {'female': 2, 'race.ethnicity': 5, 'high.educ_group': 4, 'income_group': 8, 'married': 6}\n",
    "num_cols = [x for x in list(val_df.columns) if '_norm' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hROMApYiDagm",
    "papermill": {
     "duration": 0.245307,
     "end_time": "2019-11-24T00:49:13.565462",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.320155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict):\n",
    "  for col in num_cols:\n",
    "    tmp_col = col\n",
    "    tmp_std = norm_dict[tmp_col.replace('_norm','')]['std']\n",
    "    tmp_y_true = tf.cast(y_true[col], tf.float32).numpy()\n",
    "    tmp_y_pred = np.squeeze(y_pred[col].numpy())\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    else:\n",
    "      out_loss[tmp_col] += np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "    else:\n",
    "      out_acc[tmp_col] += np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "  for col in list(cat_cols.keys()):\n",
    "    tmp_col = col\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    else:\n",
    "      out_loss[tmp_col] += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()\n",
    "    else:\n",
    "      out_acc[tmp_col] += tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()    \n",
    "  return(out_loss, out_acc)\n",
    "\n",
    "def format_output(out_loss, out_acc, n, cols, print_bl=False):\n",
    "  loss = 0\n",
    "  acc = 0\n",
    "  output = []\n",
    "  for col in cols:\n",
    "    output.append([col, out_loss[col]/n, out_acc[col]/n])\n",
    "    loss += out_loss[col]/n\n",
    "    acc += out_acc[col]/n\n",
    "  df = pd.DataFrame(output)\n",
    "  df.columns = ['name', 'loss', 'acc']\n",
    "  if print_bl:\n",
    "    print(df)\n",
    "  return(loss, acc, df)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, y, model, optimizer, cat_cols, num_cols):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(X)\n",
    "    i = 0\n",
    "    loss = tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for i in range(1,len(num_cols)):\n",
    "      loss += tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for col in list(cat_cols.keys()):\n",
    "      loss += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y[col]), tf.squeeze(predictions[col]))\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]\n",
    "  with tf.control_dependencies(mean_std):\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return(y, predictions, loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(X, y, model):\n",
    "  predictions = model(X)\n",
    "  return(y, predictions)\n",
    "\n",
    "def epoch(data_iter, df, model, optimizer, cat_cols, num_cols, norm_dict):\n",
    "  out_loss = {}\n",
    "  out_acc = {}\n",
    "  n = 0.\n",
    "  n_batch = 0.\n",
    "  total_time_dataload = 0.\n",
    "  total_time_model = 0.\n",
    "  start_time = time.time()\n",
    "  for batch in data_iter:\n",
    "    total_time_dataload += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "    t2 = (batch['t2']-t2_mean)/t2_std\n",
    "    if False:\n",
    "        ad = batch['ad']\n",
    "        ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "        ad = (ad-ad_mean)/ad_std\n",
    "        fa = batch['fa']\n",
    "        fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "        fa = (fa-fa_mean)/fa_std\n",
    "        md = batch['md']\n",
    "        md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "        md = (md-md_mean)/md_std\n",
    "        rd = batch['rd']\n",
    "        rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "        rd = (rd-rd_mean)/rd_std\n",
    "    subjectid = decoder(batch['subjectid'])\n",
    "    y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "    #X = tf.concat([t1], axis=4)\n",
    "    X = tf.concat([t1, t2], axis=4)\n",
    "    if optimizer != None:\n",
    "      y_true, y_pred, loss = train_step(X, y, model, optimizer, cat_cols, num_cols)\n",
    "    else:\n",
    "      y_true, y_pred = test_step(X, y, model)\n",
    "    out_loss, out_acc = calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict)\n",
    "    n += X.shape[0]\n",
    "    n_batch += 1\n",
    "    if (n_batch % 10) == 0:\n",
    "      log_textfile(path_output + modelname + '/log' + '.log', str(n_batch))\n",
    "    total_time_model += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "  return (out_loss, out_acc, n, total_time_model, total_time_dataload)\n",
    "\n",
    "def get_labels(df, subjectid, cols = ['nihtbx_fluidcomp_uncorrected_norm']):\n",
    "  subjects_df = pd.DataFrame(subjectid)\n",
    "  result_df = pd.merge(subjects_df, df, left_on=0, right_on='subjectkey', how='left')\n",
    "  output = {}\n",
    "  for col in cols:\n",
    "    output[col] = np.asarray(result_df[col].values)\n",
    "  return output\n",
    "\n",
    "def best_val(df_best, df_val, df_test, e):\n",
    "  df_best = pd.merge(df_best, df_val, how='left', left_on='name', right_on='name')\n",
    "  df_best = pd.merge(df_best, df_test, how='left', left_on='name', right_on='name')\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_epochs'] = e\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_epochs'] = e\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_test'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_test']\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_val'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_val']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best = df_best.drop(['cur_loss_val', 'cur_acc_val', 'cur_loss_test', 'cur_acc_test'], axis=1)\n",
    "  return(df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46355,
     "status": "ok",
     "timestamp": 1573636701683,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "RV24LE3k-00n",
    "outputId": "067125be-6ce2-40c9-998e-62f800ccb2f1",
    "papermill": {
     "duration": 0.087224,
     "end_time": "2019-11-24T00:49:13.685344",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.598120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "    template = 'Epoch {0}, Loss: {1:.3f}, Accuracy: {2:.3f}, Val Loss: {3:.3f}, Val Accuracy: {4:.3f}, Time Model: {5:.3f}, Time Data: {6:.3f}'\n",
    "    for col in [0]:\n",
    "      log_textfile(path_output + modelname + '/log' + '.log', cat_cols)\n",
    "      log_textfile(path_output + modelname + '/log' + '.log', num_cols)\n",
    "      loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "      optimizer = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "      model = Model(cat_cols, num_cols)\n",
    "      df_best = None\n",
    "      for e in range(20):\n",
    "        log_textfile(path_output + modelname + '/log' + '.log', 'Epochs: ' + str(e))\n",
    "        loss = tf.Variable(0.)\n",
    "        acc = tf.Variable(0.) \n",
    "        val_loss = tf.Variable(0.)\n",
    "        val_acc = tf.Variable(0.)\n",
    "        test_loss = tf.Variable(0.)\n",
    "        test_acc = tf.Variable(0.)\n",
    "        tf.keras.backend.set_learning_phase(True)\n",
    "        train_out_loss, train_out_acc, n, time_model, time_data = epoch(train_iter, train_df, model, optimizer, cat_cols, num_cols, norm_dict)\n",
    "        tf.keras.backend.set_learning_phase(False)\n",
    "        val_out_loss, val_out_acc, n, _, _ = epoch(val_iter, val_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "        test_out_loss, test_out_acc, n, _, _ = epoch(test_iter, test_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "        loss, acc, _ = format_output(train_out_loss, train_out_acc, n, list(cat_cols.keys())+num_cols)\n",
    "        val_loss, val_acc, df_val = format_output(val_out_loss, val_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "        test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "        df_val.columns = ['name', 'cur_loss_val', 'cur_acc_val']\n",
    "        df_test.columns = ['name', 'cur_loss_test', 'cur_acc_test']\n",
    "        if e == 0:\n",
    "          df_best = pd.merge(df_test, df_val, how='left', left_on='name', right_on='name')\n",
    "          df_best['best_acc_epochs'] = 0\n",
    "          df_best['best_loss_epochs'] = 0\n",
    "          df_best.columns = ['name', 'best_loss_test', 'best_acc_test', 'best_loss_val', 'best_acc_val', 'best_acc_epochs', 'best_loss_epochs']\n",
    "        df_best = best_val(df_best, df_val, df_test, e)\n",
    "        print(df_best[['name', 'best_loss_test', 'best_acc_test']])\n",
    "        print(df_best[['name', 'best_loss_val', 'best_acc_val']])\n",
    "        log_textfile(path_output + modelname + '/log' + '.log', template.format(e, loss, acc, val_loss, val_acc, time_model, time_data))\n",
    "        if e in [10, 15]:\n",
    "          optimizer.lr = optimizer.lr/3\n",
    "          log_textfile(path_output + modelname + '/log' + '.log', 'Learning rate: ' + str(optimizer.lr))\n",
    "        df_best.to_csv(path_output +  modelname + '/df_best' + str(e) + '.csv')\n",
    "        df_best.to_csv(path_output +  modelname + '/df_best' + '.csv')\n",
    "        model.save_weights(path_output + modelname + '/checkpoints/' + str(e) + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "papermill": {
     "duration": 0.073495,
     "end_time": "2019-11-24T00:49:13.790933",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.717438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "papermill": {
     "duration": 0.209926,
     "end_time": "2019-11-24T00:49:14.032144",
     "exception": false,
     "start_time": "2019-11-24T00:49:13.822218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "papermill": {
     "duration": 0.204795,
     "end_time": "2019-11-24T00:49:14.268521",
     "exception": false,
     "start_time": "2019-11-24T00:49:14.063726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fc64829d9b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = 16\n",
    "model.load_weights(path_output + modelname + '/checkpoints/' + str(e) + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "papermill": {
     "duration": 0.076689,
     "end_time": "2019-11-24T00:49:14.377989",
     "exception": false,
     "start_time": "2019-11-24T00:49:14.301300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "papermill": {
     "duration": 48.777737,
     "end_time": "2019-11-24T00:50:03.187809",
     "exception": false,
     "start_time": "2019-11-24T00:49:14.410072",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.0\n"
     ]
    }
   ],
   "source": [
    "test_out_loss, test_out_acc, n, _, _ = epoch(test_iter, test_df, model, None, cat_cols, num_cols, norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "papermill": {
     "duration": 0.084666,
     "end_time": "2019-11-24T00:50:03.310088",
     "exception": false,
     "start_time": "2019-11-24T00:50:03.225422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 2.585685,
     "end_time": "2019-11-24T00:50:05.932535",
     "exception": false,
     "start_time": "2019-11-24T00:50:03.346850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(64,64,64,2), name='inputlayer123')\n",
    "a = model(inputs)['female']\n",
    "mm = tf.keras.models.Model(inputs=inputs, outputs=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "papermill": {
     "duration": 0.252854,
     "end_time": "2019-11-24T00:50:06.223927",
     "exception": false,
     "start_time": "2019-11-24T00:50:05.971073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_explain.core.smoothgrad import SmoothGrad\n",
    "import pickle\n",
    "\n",
    "if False:\n",
    "    explainer = SmoothGrad()\n",
    "    output_grid = {}\n",
    "    output_n = {}\n",
    "    output_grids = []\n",
    "    for i in range(2):\n",
    "      output_grid[i] = np.zeros((64,64,64))\n",
    "      output_n[i] = 0\n",
    "    counter = 0\n",
    "    for batch in test_iter:\n",
    "      counter+=1\n",
    "      print(counter)\n",
    "      t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "      t2 = (batch['t2']-t2_mean)/t2_std\n",
    "      X = tf.concat([t1, t2], axis=4)\n",
    "      subjectid = decoder(batch['subjectid'])\n",
    "      y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "      y_list = list(y['female'])\n",
    "      for i in range(X.shape[0]):\n",
    "        X_i = X[i]\n",
    "        X_i = tf.expand_dims(X_i, axis=0)\n",
    "        y_i = y_list[i]\n",
    "        grid = explainer.explain((X_i, _), mm, y_i, 20, 1.)\n",
    "        output_grid[y_i] += grid\n",
    "        output_n[y_i] += 1\n",
    "        output_grids.append([grid, y_i])\n",
    "      if counter == 6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 0.081667,
     "end_time": "2019-11-24T00:50:06.342557",
     "exception": false,
     "start_time": "2019-11-24T00:50:06.260890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pickle.dump([output_grid, output_n, output_grids], open(path_output + modelname + \"smoothgrad_female_all.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 0.077088,
     "end_time": "2019-11-24T00:50:06.456640",
     "exception": false,
     "start_time": "2019-11-24T00:50:06.379552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size):\n",
    "    \"\"\"\n",
    "    Replace a part of the image with a grey patch.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image\n",
    "        top_left_x (int): Top Left X position of the applied box\n",
    "        top_left_y (int): Top Left Y position of the applied box\n",
    "        patch_size (int): Size of patch to apply\n",
    "    Returns:\n",
    "        numpy.ndarray: Patched image\n",
    "    \"\"\"\n",
    "    patched_image = np.array(image, copy=True)\n",
    "    patched_image[\n",
    "        top_left_x : top_left_x + patch_size, top_left_y : top_left_y + patch_size, top_left_z : top_left_z + patch_size, :\n",
    "    ] = 0\n",
    "\n",
    "    return patched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "papermill": {
     "duration": 0.078669,
     "end_time": "2019-11-24T00:50:06.572767",
     "exception": false,
     "start_time": "2019-11-24T00:50:06.494098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "papermill": {
     "duration": 0.083942,
     "end_time": "2019-11-24T00:50:06.694586",
     "exception": false,
     "start_time": "2019-11-24T00:50:06.610644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sensgrid(image, mm, class_index, patch_size):\n",
    "  sensitivity_map = np.zeros((\n",
    "    math.ceil(image.shape[0] / patch_size),\n",
    "    math.ceil(image.shape[1] / patch_size),\n",
    "    math.ceil(image.shape[2] / patch_size)\n",
    "  ))\n",
    "  for index_z, top_left_z in enumerate(range(0, image.shape[2], patch_size)):\n",
    "    patches = [\n",
    "              apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size)\n",
    "              for index_x, top_left_x in enumerate(range(0, image.shape[0], patch_size))\n",
    "              for index_y, top_left_y in enumerate(range(0, image.shape[1], patch_size))\n",
    "              ]\n",
    "    coordinates = [\n",
    "                (index_y, index_x)\n",
    "                for index_x, _ in enumerate(range(0, image.shape[0], patch_size))\n",
    "                for index_y, _ in enumerate(range(0, image.shape[1], patch_size))\n",
    "                ]\n",
    "    predictions = mm.predict(np.array(patches), batch_size=1)\n",
    "    target_class_predictions = [prediction[class_index] for prediction in predictions]\n",
    "    for (index_y, index_x), confidence in zip(coordinates, target_class_predictions):\n",
    "      sensitivity_map[index_y, index_x, index_z] = 1 - confidence\n",
    "  sm = resize(sensitivity_map, (64,64,64))\n",
    "  heatmap = (sm - np.min(sm)) / (sm.max() - sm.min())\n",
    "  return(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 8278.108553,
     "end_time": "2019-11-24T03:08:04.840552",
     "exception": false,
     "start_time": "2019-11-24T00:50:06.731999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n",
      "Learning rate: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n",
      "Learning rate: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n",
      "Learning rate: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n",
      "Learning rate: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n",
      "Learning rate: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n",
      "Learning rate: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 7\n"
     ]
    }
   ],
   "source": [
    "output_grid = {}\n",
    "output_n = {}\n",
    "output_grids = []\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  log_textfile(path_output + modelname + '/log_occ' + '.log', 'Learning rate: ' + str(counter))\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    log_textfile(path_output + modelname + '/log_occ' + '.log', 'Learning rate: ' + str(i))\n",
    "    X_i = X[i]\n",
    "    y_i = y_list[i]\n",
    "    grid = get_sensgrid(X_i, mm, y_i, 4)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1\n",
    "    output_grids.append([grid, y_i])\n",
    "  if counter==6:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "papermill": {
     "duration": 4.228615,
     "end_time": "2019-11-24T03:08:09.124085",
     "exception": false,
     "start_time": "2019-11-24T03:08:04.895470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc1c5425bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc1c5425bf8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc1c5412510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc1c5412510> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc1c54249d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc1c54249d8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc1c4247400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc1c4247400> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7fc1c416f0d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7fc1c416f0d0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    }
   ],
   "source": [
    "pickle.dump([output_grid, output_n, output_grids], open(path_output + modelname + \"occsensitivity_female_all.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.052941,
     "end_time": "2019-11-24T03:08:09.230800",
     "exception": false,
     "start_time": "2019-11-24T03:08:09.177859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.053952,
     "end_time": "2019-11-24T03:08:09.339824",
     "exception": false,
     "start_time": "2019-11-24T03:08:09.285872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "papermill": {
     "duration": 0.384312,
     "end_time": "2019-11-24T03:08:09.777681",
     "exception": true,
     "start_time": "2019-11-24T03:08:09.393369",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ded28046d863>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test.to_csv('final_output_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(64,64,64,2), name='inputlayer123')\n",
    "a = model(inputs)['female']\n",
    "mm = tf.keras.models.Model(inputs=inputs, outputs=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tf_explain.core.smoothgrad import SmoothGrad\n",
    "import pickle\n",
    "\n",
    "explainer = SmoothGrad()\n",
    "output_grid = {}\n",
    "output_n = {}\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  print(counter)\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    X_i = X[i]\n",
    "    X_i = tf.expand_dims(X_i, axis=0)\n",
    "    y_i = y_list[i]\n",
    "    grid = explainer.explain((X_i, _), mm, y_i, 20, 1.)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump([output_grid, output_n], open( \"smoothgrad_female_all.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#output_grid, output_n = pickle.load(open( \"smoothgrad_female.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size):\n",
    "    \"\"\"\n",
    "    Replace a part of the image with a grey patch.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image\n",
    "        top_left_x (int): Top Left X position of the applied box\n",
    "        top_left_y (int): Top Left Y position of the applied box\n",
    "        patch_size (int): Size of patch to apply\n",
    "    Returns:\n",
    "        numpy.ndarray: Patched image\n",
    "    \"\"\"\n",
    "    patched_image = np.array(image, copy=True)\n",
    "    patched_image[\n",
    "        top_left_x : top_left_x + patch_size, top_left_y : top_left_y + patch_size, top_left_z : top_left_z + patch_size, :\n",
    "    ] = 0\n",
    "\n",
    "    return patched_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sensgrid(image, mm, class_index, patch_size):\n",
    "  sensitivity_map = np.zeros((\n",
    "    math.ceil(image.shape[0] / patch_size),\n",
    "    math.ceil(image.shape[1] / patch_size),\n",
    "    math.ceil(image.shape[2] / patch_size)\n",
    "  ))\n",
    "  for index_z, top_left_z in enumerate(range(0, image.shape[2], patch_size)):\n",
    "    patches = [\n",
    "              apply_grey_patch(image, top_left_x, top_left_y, top_left_z, patch_size)\n",
    "              for index_x, top_left_x in enumerate(range(0, image.shape[0], patch_size))\n",
    "              for index_y, top_left_y in enumerate(range(0, image.shape[1], patch_size))\n",
    "              ]\n",
    "    coordinates = [\n",
    "                (index_y, index_x)\n",
    "                for index_x, _ in enumerate(range(0, image.shape[0], patch_size))\n",
    "                for index_y, _ in enumerate(range(0, image.shape[1], patch_size))\n",
    "                ]\n",
    "    predictions = mm.predict(np.array(patches), batch_size=1)\n",
    "    target_class_predictions = [prediction[class_index] for prediction in predictions]\n",
    "    for (index_y, index_x), confidence in zip(coordinates, target_class_predictions):\n",
    "      sensitivity_map[index_y, index_x, index_z] = 1 - confidence\n",
    "  sm = resize(sensitivity_map, (64,64,64))\n",
    "  heatmap = (sm - np.min(sm)) / (sm.max() - sm.min())\n",
    "  return(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_grid = {}\n",
    "output_n = {}\n",
    "for i in range(2):\n",
    "  output_grid[i] = np.zeros((64,64,64))\n",
    "  output_n[i] = 0\n",
    "\n",
    "counter = 0\n",
    "for batch in test_iter:\n",
    "  counter+=1\n",
    "  print(counter)\n",
    "  t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "  t2 = (batch['t2']-t2_mean)/t2_std\n",
    "  X = tf.concat([t1, t2], axis=4)\n",
    "  subjectid = decoder(batch['subjectid'])\n",
    "  y = get_labels(test_df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "  y_list = list(y['female'])\n",
    "  for i in range(X.shape[0]):\n",
    "    print(i)\n",
    "    X_i = X[i]\n",
    "    y_i = y_list[i]\n",
    "    grid = get_sensgrid(X_i, mm, y_i, 4)\n",
    "    output_grid[y_i] += grid\n",
    "    output_n[y_i] += 1\n",
    "  if counter==6:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump([output_grid, output_n], open( \"heatmap_female_all.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "t2 = (batch['t2']-t2_mean)/t2_std\n",
    "ad = batch['ad']\n",
    "ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "ad = (ad-ad_mean)/ad_std\n",
    "fa = batch['fa']\n",
    "fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "fa = (fa-fa_mean)/fa_std\n",
    "md = batch['md']\n",
    "md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "md = (md-md_mean)/md_std\n",
    "rd = batch['rd']\n",
    "rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "rd = (rd-rd_mean)/rd_std\n",
    "#subjectid = decoder(batch['subjectid'])\n",
    "#y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "#X = tf.concat([t1, t2, ad, fa, md, rd], axis=4)\n",
    "X = tf.concat([t1, t2], axis=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(False)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.non_trainable_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "06_SimpleDL_MultiTask_Valid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "papermill": {
   "duration": 8379.09215,
   "end_time": "2019-11-24T03:08:13.588099",
   "environment_variables": {},
   "exception": true,
   "input_path": "RunModels_run_down64_DenseNet_explain.ipynb",
   "output_path": "xxx_out.ipynb",
   "parameters": {},
   "start_time": "2019-11-24T00:48:34.495949",
   "version": "1.2.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}