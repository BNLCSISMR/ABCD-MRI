{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 48054,
     "status": "ok",
     "timestamp": 1573636583219,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "2mU_ZLUH5jz4",
    "outputId": "c4cbc445-56dd-49df-de87-c34e18da5f6c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import nibabel as nib\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from mricode.utils import log_textfile\n",
    "from mricode.utils import copy_colab\n",
    "from mricode.utils import return_iter\n",
    "from mricode.utils import return_csv\n",
    "\n",
    "from mricode.models.SimpleCNN import SimpleCNN\n",
    "from mricode.models.DenseNet import MyDenseNet\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow import nn\n",
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.keras.engine.base_layer import InputSpec\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nH4XzW8C5yhH"
   },
   "outputs": [],
   "source": [
    "path_output = './output/'\n",
    "path_tfrecords = '/data2/res64/down/'\n",
    "path_csv = '/data2/csv/'\n",
    "filename_res = {'train': 'intell_residual_train.csv', 'val': 'intell_residual_valid.csv', 'test': 'intell_residual_test.csv'}\n",
    "filename_final = filename_res\n",
    "sample_size = 'allimages'\n",
    "batch_size = 8\n",
    "onlyt1 = False\n",
    "modelname = 'runAllImages64_DenseNet_T1T2_'\n",
    "Model = SimpleCNN\n",
    "Model = MyDenseNet\n",
    "t1_mean=1.3779395849814497\n",
    "t1_std=3.4895845243139503\n",
    "t2_mean=2.22435586968901\n",
    "t2_std=5.07708743178319\n",
    "ad_mean=1.3008901218593748e-05\n",
    "ad_std=0.009966655860940228\n",
    "fa_mean=0.0037552628409334037\n",
    "fa_std=0.012922319568740915\n",
    "md_mean=9.827903909139596e-06\n",
    "md_std=0.009956973204022659\n",
    "rd_mean=8.237404999587111e-06\n",
    "rd_std=0.009954672598675338"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4520,
     "status": "ok",
     "timestamp": 1573636640714,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "B-rSS9vT6TuF",
    "outputId": "f5b43cbc-7d59-4c7c-b6d6-78882b1ef3e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <function return_iter.<locals>._parse_ at 0x7f5978b37d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n",
      "WARNING: Entity <function return_iter.<locals>._parse_ at 0x7f5978b37d08> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: \n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter, test_iter = return_iter(path_tfrecords, sample_size, batch_size, onlyt1=onlyt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJ0VnyEzBAes"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-99aed77945cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mrd_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0mt1_mean\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mt1_std\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 622\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;34m\"\"\"Returns a nested structure of `Tensor`s containing the next element.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m         \u001b[0;34m\"IteratorGetNextSync\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m         \"output_types\", output_types, \"output_shapes\", output_shapes)\n\u001b[0m\u001b[1;32m   2660\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if False:\n",
    "  t1_mean = 0.\n",
    "  t1_std = 0.\n",
    "  t2_mean = 0.\n",
    "  t2_std = 0.\n",
    "  ad_mean = 0.\n",
    "  ad_std = 0.\n",
    "  fa_mean = 0.\n",
    "  fa_std = 0.\n",
    "  md_mean = 0.\n",
    "  md_std = 0.\n",
    "  rd_mean = 0.\n",
    "  rd_std = 0.\n",
    "  n = 0.\n",
    "  for b in train_iter:\n",
    "      t1_mean += np.mean(b['t1'])\n",
    "      t1_std += np.std(b['t1'])\n",
    "      t2_mean += np.mean(b['t2'])\n",
    "      t2_std += np.std(b['t2'])\n",
    "      a = np.asarray(b['ad'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      ad_mean += np.mean(a)\n",
    "      ad_std += np.std(a)\n",
    "      a = np.asarray(b['fa'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      fa_mean += np.mean(a)\n",
    "      fa_std += np.std(a)\n",
    "      a = np.asarray(b['md'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      md_mean += np.mean(a)\n",
    "      md_std += np.std(a)\n",
    "      a = np.asarray(b['rd'])\n",
    "      a = a.copy()\n",
    "      a[np.isnan(a)] = 0\n",
    "      rd_mean += np.mean(a)\n",
    "      rd_std += np.std(a)\n",
    "      n += np.asarray(b['t1']).shape[0]\n",
    "\n",
    "  t1_mean /= n\n",
    "  t1_std /= n\n",
    "  t2_mean /= n\n",
    "  t2_std /= n\n",
    "  ad_mean /= n\n",
    "  ad_std /= n\n",
    "  fa_mean /= n\n",
    "  fa_std /= n\n",
    "  md_mean /= n\n",
    "  md_std /= n\n",
    "  rd_mean /= n\n",
    "  rd_std /= n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2924,
     "status": "ok",
     "timestamp": 1573636640716,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "mSwhEqDPM_r8",
    "outputId": "14a4ac0b-9587-4776-fc64-1e008f0c5c31"
   },
   "outputs": [],
   "source": [
    "t1_mean, t1_std, t2_mean, t2_std, ad_mean, ad_std, fa_mean, fa_std, md_mean, md_std, rd_mean, rd_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PAyV5ktlA6K1"
   },
   "outputs": [],
   "source": [
    "train_df, val_df, test_df, norm_dict = return_csv(path_csv, filename_final, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3147,
     "status": "ok",
     "timestamp": 1573636641787,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "0xYy4XUyVBeC",
    "outputId": "a6a19d59-31ff-4064-f1dc-952befcce4b3"
   },
   "outputs": [],
   "source": [
    "norm_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTk95ptFV5oN"
   },
   "outputs": [],
   "source": [
    "cat_cols = {'female': 2, 'race.ethnicity': 5, 'high.educ_group': 4, 'income_group': 8, 'married': 6}\n",
    "num_cols = [x for x in list(val_df.columns) if '_norm' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hROMApYiDagm"
   },
   "outputs": [],
   "source": [
    "def calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict):\n",
    "  for col in num_cols:\n",
    "    tmp_col = col\n",
    "    tmp_std = norm_dict[tmp_col.replace('_norm','')]['std']\n",
    "    tmp_y_true = tf.cast(y_true[col], tf.float32).numpy()\n",
    "    tmp_y_pred = np.squeeze(y_pred[col].numpy())\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    else:\n",
    "      out_loss[tmp_col] += np.sum(np.square(tmp_y_true-tmp_y_pred))\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "    else:\n",
    "      out_acc[tmp_col] += np.sum(np.square((tmp_y_true-tmp_y_pred)*tmp_std))\n",
    "  for col in list(cat_cols.keys()):\n",
    "    tmp_col = col\n",
    "    if not(tmp_col in out_loss):\n",
    "      out_loss[tmp_col] = tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    else:\n",
    "      out_loss[tmp_col] += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y_true[col]), tf.squeeze(y_pred[col])).numpy()\n",
    "    if not(tmp_col in out_acc):\n",
    "      out_acc[tmp_col] = tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()\n",
    "    else:\n",
    "      out_acc[tmp_col] += tf.reduce_sum(tf.dtypes.cast((y_true[col] == tf.argmax(y_pred[col], axis=-1)), tf.float32)).numpy()    \n",
    "  return(out_loss, out_acc)\n",
    "\n",
    "def format_output(out_loss, out_acc, n, cols, print_bl=False):\n",
    "  loss = 0\n",
    "  acc = 0\n",
    "  output = []\n",
    "  for col in cols:\n",
    "    output.append([col, out_loss[col]/n, out_acc[col]/n])\n",
    "    loss += out_loss[col]/n\n",
    "    acc += out_acc[col]/n\n",
    "  df = pd.DataFrame(output)\n",
    "  df.columns = ['name', 'loss', 'acc']\n",
    "  if print_bl:\n",
    "    print(df)\n",
    "  return(loss, acc, df)\n",
    "\n",
    "@tf.function\n",
    "def train_step(X, y, model, optimizer, cat_cols, num_cols):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(X)\n",
    "    i = 0\n",
    "    loss = tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for i in range(1,len(num_cols)):\n",
    "      loss += tf.keras.losses.MSE(tf.cast(y[num_cols[i]], tf.float32), tf.squeeze(predictions[num_cols[i]]))\n",
    "    for col in list(cat_cols.keys()):\n",
    "      loss += tf.keras.losses.SparseCategoricalCrossentropy()(tf.squeeze(y[col]), tf.squeeze(predictions[col]))\n",
    "  gradients = tape.gradient(loss, model.trainable_variables)\n",
    "  mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]\n",
    "  with tf.control_dependencies(mean_std):\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "  return(y, predictions, loss)\n",
    "\n",
    "@tf.function\n",
    "def test_step(X, y, model):\n",
    "  predictions = model(X)\n",
    "  return(y, predictions)\n",
    "\n",
    "def epoch(data_iter, df, model, optimizer, cat_cols, num_cols, norm_dict):\n",
    "  out_loss = {}\n",
    "  out_acc = {}\n",
    "  n = 0.\n",
    "  n_batch = 0.\n",
    "  total_time_dataload = 0.\n",
    "  total_time_model = 0.\n",
    "  start_time = time.time()\n",
    "  for batch in data_iter:\n",
    "    total_time_dataload += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "    t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "    t2 = (batch['t2']-t2_mean)/t2_std\n",
    "    ad = batch['ad']\n",
    "    ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "    ad = (ad-ad_mean)/ad_std\n",
    "    fa = batch['fa']\n",
    "    fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "    fa = (fa-fa_mean)/fa_std\n",
    "    md = batch['md']\n",
    "    md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "    md = (md-md_mean)/md_std\n",
    "    rd = batch['rd']\n",
    "    rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "    rd = (rd-rd_mean)/rd_std\n",
    "    subjectid = decoder(batch['subjectid'])\n",
    "    y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "    X = tf.concat([t1, t2], axis=4)\n",
    "    #X = tf.concat([t1, t2], axis=4)\n",
    "    if optimizer != None:\n",
    "      y_true, y_pred, loss = train_step(X, y, model, optimizer, cat_cols, num_cols)\n",
    "    else:\n",
    "      y_true, y_pred = test_step(X, y, model)\n",
    "    out_loss, out_acc = calc_loss_acc(out_loss, out_acc, y_true, y_pred, cat_cols, num_cols, norm_dict)\n",
    "    n += X.shape[0]\n",
    "    n_batch += 1\n",
    "    if (n_batch % 10) == 0:\n",
    "      print(n_batch)\n",
    "    total_time_model += time.time() - start_time\n",
    "    start_time = time.time()\n",
    "  return (out_loss, out_acc, n, total_time_model, total_time_dataload)\n",
    "\n",
    "def get_labels(df, subjectid, cols = ['nihtbx_fluidcomp_uncorrected_norm']):\n",
    "  subjects_df = pd.DataFrame(subjectid)\n",
    "  result_df = pd.merge(subjects_df, df, left_on=0, right_on='subjectkey', how='left')\n",
    "  output = {}\n",
    "  for col in cols:\n",
    "    output[col] = np.asarray(result_df[col].values)\n",
    "  return output\n",
    "\n",
    "def best_val(df_best, df_val, df_test):\n",
    "  df_best = pd.merge(df_best, df_val, how='left', left_on='name', right_on='name')\n",
    "  df_best = pd.merge(df_best, df_test, how='left', left_on='name', right_on='name')\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_test'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_test']\n",
    "  df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'best_loss_val'] = df_best.loc[df_best['best_loss_val']>=df_best['cur_loss_val'], 'cur_loss_val']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']<=df_best['cur_acc_val'])&(df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_test'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_test']\n",
    "  df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'best_acc_val'] = df_best.loc[(df_best['best_acc_val']>=df_best['cur_acc_val'])&(~df_best['name'].isin(['female', 'race.ethnicity', 'high.educ_group', 'income_group', 'married'])), 'cur_acc_val']\n",
    "  df_best = df_best.drop(['cur_loss_val', 'cur_acc_val', 'cur_loss_test', 'cur_acc_test'], axis=1)\n",
    "  return(df_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 46355,
     "status": "ok",
     "timestamp": 1573636701683,
     "user": {
      "displayName": "Benedikt Dietmar Schifferer",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDViwxKXBjVKZfXMWGuXrQ48D62bye6HNutAOX0=s64",
      "userId": "06737229821528734971"
     },
     "user_tz": -60
    },
    "id": "RV24LE3k-00n",
    "outputId": "067125be-6ce2-40c9-998e-62f800ccb2f1"
   },
   "outputs": [],
   "source": [
    "decoder = np.vectorize(lambda x: x.decode('UTF-8'))\n",
    "template = 'Epoch {0}, Loss: {1:.3f}, Accuracy: {2:.3f}, Val Loss: {3:.3f}, Val Accuracy: {4:.3f}, Time Model: {5:.3f}, Time Data: {6:.3f}'\n",
    "for col in [0]:\n",
    "  log_textfile(path_output + modelname + 'multitask_test' + '.log', cat_cols),\n",
    "  log_textfile(path_output + modelname + 'multitask_test' + '.log', num_cols)\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "  optimizer = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "  model = Model(cat_cols, num_cols)\n",
    "  df_best = None\n",
    "  for e in range(10):\n",
    "    log_textfile(path_output + modelname + 'multitask_test' + '.log', 'Epochs: ' + str(e))\n",
    "    loss = tf.Variable(0.)\n",
    "    acc = tf.Variable(0.) \n",
    "    val_loss = tf.Variable(0.)\n",
    "    val_acc = tf.Variable(0.)\n",
    "    test_loss = tf.Variable(0.)\n",
    "    test_acc = tf.Variable(0.)\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "    train_out_loss, train_out_acc, n, time_model, time_data = epoch(train_iter, train_df, model, optimizer, cat_cols, num_cols, norm_dict)\n",
    "    tf.keras.backend.set_learning_phase(False)\n",
    "    val_out_loss, val_out_acc, n, _, _ = epoch(val_iter, val_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "    test_out_loss, test_out_acc, n, _, _ = epoch(test_iter, test_df, model, None, cat_cols, num_cols, norm_dict)\n",
    "    loss, acc, _ = format_output(train_out_loss, train_out_acc, n, list(cat_cols.keys())+num_cols)\n",
    "    val_loss, val_acc, df_val = format_output(val_out_loss, val_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "    test_loss, test_acc, df_test = format_output(test_out_loss, test_out_acc, n, list(cat_cols.keys())+num_cols, print_bl=False)\n",
    "    df_val.columns = ['name', 'cur_loss_val', 'cur_acc_val']\n",
    "    df_test.columns = ['name', 'cur_loss_test', 'cur_acc_test']\n",
    "    if e == 0:\n",
    "      df_best = pd.merge(df_test, df_val, how='left', left_on='name', right_on='name')\n",
    "      df_best.columns = ['name', 'best_loss_test', 'best_acc_test', 'best_loss_val', 'best_acc_val']\n",
    "    df_best = best_val(df_best, df_val, df_test)\n",
    "    print(df_best[['name', 'best_loss_test', 'best_acc_test']])\n",
    "    print(df_best[['name', 'best_loss_val', 'best_acc_val']])\n",
    "    log_textfile(path_output +  modelname + 'multitask_test' + '.log', template.format(e, loss, acc, val_loss, val_acc, time_model, time_data))\n",
    "    if e in [7, 16]:\n",
    "      optimizer.lr = optimizer.lr/3\n",
    "      log_textfile(path_output +  modelname + 'multitask_test' + '.log', 'Learning rate: ' + str(optimizer.lr))\n",
    "    df_best.to_csv(path_output +  modelname + 'multitask_test' + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = (tf.cast(batch['t1'], tf.float32)-t1_mean)/t1_std\n",
    "t2 = (batch['t2']-t2_mean)/t2_std\n",
    "ad = batch['ad']\n",
    "ad = tf.where(tf.math.is_nan(ad), tf.zeros_like(ad), ad)\n",
    "ad = (ad-ad_mean)/ad_std\n",
    "fa = batch['fa']\n",
    "fa = tf.where(tf.math.is_nan(fa), tf.zeros_like(fa), fa)\n",
    "fa = (fa-fa_mean)/fa_std\n",
    "md = batch['md']\n",
    "md = tf.where(tf.math.is_nan(md), tf.zeros_like(md), md)\n",
    "md = (md-md_mean)/md_std\n",
    "rd = batch['rd']\n",
    "rd = tf.where(tf.math.is_nan(rd), tf.zeros_like(rd), rd)\n",
    "rd = (rd-rd_mean)/rd_std\n",
    "#subjectid = decoder(batch['subjectid'])\n",
    "#y = get_labels(df, subjectid, list(cat_cols.keys())+num_cols)\n",
    "#X = tf.concat([t1, t2, ad, fa, md, rd], axis=4)\n",
    "X = tf.concat([t1, t2], axis=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=104053, shape=(8, 2), dtype=float32, numpy=\n",
       "array([[0.7318841 , 0.26811588],\n",
       "       [0.3975592 , 0.6024408 ],\n",
       "       [0.08681537, 0.91318464],\n",
       "       [0.8014379 , 0.19856213],\n",
       "       [0.42574984, 0.5742502 ],\n",
       "       [0.85568553, 0.1443145 ],\n",
       "       [0.3161926 , 0.68380743],\n",
       "       [0.8359293 , 0.16407076]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=104651, shape=(8, 2), dtype=float32, numpy=\n",
       "array([[0.09747502, 0.902525  ],\n",
       "       [0.08380887, 0.91619116],\n",
       "       [0.07291744, 0.92708254],\n",
       "       [0.11647341, 0.8835266 ],\n",
       "       [0.0863044 , 0.9136956 ],\n",
       "       [0.11506222, 0.88493776],\n",
       "       [0.08197798, 0.91802204],\n",
       "       [0.10716236, 0.89283764]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.backend.set_learning_phase(False)\n",
    "model(X)['female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std = [x.name for x in model.non_trainable_variables if ('batch_norm') in x.name and ('mean' in x.name or 'variance' in x.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(cat_cols, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.non_trainable_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "06_SimpleDL_MultiTask_Valid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
